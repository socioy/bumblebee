{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Training Notebook  \n",
    "\n",
    "This notebook trains an AI model to predict mouse cursor movement paths. The model is built using a Recurrent Neural Network (RNN) with an LSTM layer for attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/arpanbhandari/Documents/coding/bumblebee/data/processed/cleaned-data-39-steps-merged-prepared-data-2025-03-08-16:27:23.json exists: Yes\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(\n",
    "    os.path.dirname(os.getcwd()),\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"cleaned-data-39-steps-merged-prepared-data-2025-03-08-16:27:23.json\",\n",
    ")  # this path is for the cleaned data; must be changed accordingly\n",
    "\n",
    "print(\n",
    "    f\"{dataset_path} exists: {\"Yes\" if os.path.exists(dataset_path) else 'No'}\"\n",
    ")  ## must be Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = open(dataset_path, \"r\")\n",
    "dataset_json = json.load(dataset_file)\n",
    "dataset_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_coordinate = 0\n",
    "max_coordinate = 1920  # the maximum display resolution used is 1920x1080 so taking 1920 as the max coordinate\n",
    "\n",
    "\n",
    "def normalize(data):  # Normalizing the data to be in the range [0, 1]\n",
    "    return (data - min_coordinate) / (max_coordinate - min_coordinate)\n",
    "\n",
    "\n",
    "def denormalize(\n",
    "    data,\n",
    "):  # Denormalizing the data to convert it back to the original range\n",
    "    return abs((data * (max_coordinate - min_coordinate)) + min_coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array(dataset_json[\"input\"], dtype=np.float32)\n",
    "output_data = np.array(dataset_json[\"output\"], dtype=np.float32)\n",
    "\n",
    "input_data = normalize(input_data)\n",
    "output_data = normalize(output_data)\n",
    "\n",
    "intermediate_steps_num = output_data.shape[1]\n",
    "\n",
    "X_tensor = torch.tensor(input_data, dtype=torch.float, device=device)\n",
    "y_tensor = torch.tensor(output_data, dtype=torch.float, device=device)\n",
    "\n",
    "X_tensor = X_tensor.unsqueeze(1)\n",
    "y_tensor = y_tensor = y_tensor.view(\n",
    "    -1, 2 * intermediate_steps_num\n",
    ")  # Flattening the output tensor, 2 is used because only x, y corrdinates are needed to be predicted for each step\n",
    "\n",
    "\n",
    "del input_data, output_data  # to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(\n",
    "            hidden_dim, 1, bias=False\n",
    "        )  # Attention layer to assign weights to different time steps\n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        scores = self.attn(lstm_out)  # Compute attention scores for each time step\n",
    "        attn_weights = torch.softmax(\n",
    "            scores, dim=1\n",
    "        )  # Apply softmax to normalize attention weights\n",
    "        context = torch.sum(\n",
    "            attn_weights * lstm_out, dim=1\n",
    "        )  # Create context vector by weighted sum of LSTM outputs\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class CursorRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.2):\n",
    "        super(CursorRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=dropout,\n",
    "        )  # LSTM layer for sequence processing\n",
    "        self.attention = Attention(\n",
    "            hidden_dim\n",
    "        )  # Attention mechanism to focus on important time steps\n",
    "        self.residual_fc = nn.Linear(\n",
    "            input_dim, hidden_dim\n",
    "        )  # Residual connection to help with gradient flow\n",
    "        self.layer_norm = nn.LayerNorm(\n",
    "            hidden_dim\n",
    "        )  # Layer normalization for training stability\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_dim, output_dim\n",
    "        )  # Output projection layer to generate final predictions\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # Process sequence through LSTM\n",
    "        context, attn_weights = self.attention(\n",
    "            lstm_out\n",
    "        )  # Apply attention to focus on relevant parts\n",
    "        residual = self.residual_fc(\n",
    "            x[:, -1, :]\n",
    "        )  # Create residual connection from last input\n",
    "        combined = self.layer_norm(\n",
    "            context + residual\n",
    "        )  # Combine attention output with residual and normalize\n",
    "        output = self.fc(combined)  # Generate final trajectory prediction\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_tensor.shape[2]\n",
    "output_size = y_tensor.shape[1]\n",
    "\n",
    "hidden_size = (input_size**2) * int(output_size ** (1 / 2)) * 4\n",
    "epochs = 500\n",
    "lstm_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, num_workers=0, pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/bumblebee/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500, Loss: 0.0209469990, LR: 0.0010000000\n",
      "Epoch: 2/500, Loss: 0.0171501675, LR: 0.0010000000\n",
      "Epoch: 3/500, Loss: 0.0168515039, LR: 0.0010000000\n",
      "Epoch: 4/500, Loss: 0.0168596036, LR: 0.0010000000\n",
      "Epoch: 5/500, Loss: 0.0165182651, LR: 0.0010000000\n",
      "Epoch: 6/500, Loss: 0.0164684822, LR: 0.0010000000\n",
      "Epoch: 7/500, Loss: 0.0163356690, LR: 0.0010000000\n",
      "Epoch: 8/500, Loss: 0.0161936691, LR: 0.0010000000\n",
      "Epoch: 9/500, Loss: 0.0160396818, LR: 0.0010000000\n",
      "Epoch: 10/500, Loss: 0.0159525930, LR: 0.0010000000\n",
      "Epoch: 11/500, Loss: 0.0159610257, LR: 0.0010000000\n",
      "Epoch: 12/500, Loss: 0.0159389099, LR: 0.0010000000\n",
      "Epoch: 13/500, Loss: 0.0158358307, LR: 0.0010000000\n",
      "Epoch: 14/500, Loss: 0.0158210883, LR: 0.0010000000\n",
      "Epoch: 15/500, Loss: 0.0157875942, LR: 0.0010000000\n",
      "Epoch: 16/500, Loss: 0.0156862734, LR: 0.0010000000\n",
      "Epoch: 17/500, Loss: 0.0156520209, LR: 0.0010000000\n",
      "Epoch: 18/500, Loss: 0.0156497353, LR: 0.0010000000\n",
      "Epoch: 19/500, Loss: 0.0156090618, LR: 0.0010000000\n",
      "Epoch: 20/500, Loss: 0.0155704508, LR: 0.0010000000\n",
      "Epoch: 21/500, Loss: 0.0155132247, LR: 0.0010000000\n",
      "Epoch: 22/500, Loss: 0.0154485622, LR: 0.0010000000\n",
      "Epoch: 23/500, Loss: 0.0155169129, LR: 0.0010000000\n",
      "Epoch: 24/500, Loss: 0.0154694981, LR: 0.0010000000\n",
      "Epoch: 25/500, Loss: 0.0153757980, LR: 0.0010000000\n",
      "Epoch: 26/500, Loss: 0.0153825730, LR: 0.0010000000\n",
      "Epoch: 27/500, Loss: 0.0153657173, LR: 0.0010000000\n",
      "Epoch: 28/500, Loss: 0.0153453002, LR: 0.0010000000\n",
      "Epoch: 29/500, Loss: 0.0153189059, LR: 0.0010000000\n",
      "Epoch: 30/500, Loss: 0.0153491492, LR: 0.0010000000\n",
      "Epoch: 31/500, Loss: 0.0152989476, LR: 0.0010000000\n",
      "Epoch: 32/500, Loss: 0.0153170184, LR: 0.0010000000\n",
      "Epoch: 33/500, Loss: 0.0152541156, LR: 0.0010000000\n",
      "Epoch: 34/500, Loss: 0.0152510368, LR: 0.0010000000\n",
      "Epoch: 35/500, Loss: 0.0152412986, LR: 0.0010000000\n",
      "Epoch: 36/500, Loss: 0.0152173380, LR: 0.0010000000\n",
      "Epoch: 37/500, Loss: 0.0152701273, LR: 0.0010000000\n",
      "Epoch: 38/500, Loss: 0.0152260049, LR: 0.0010000000\n",
      "Epoch: 39/500, Loss: 0.0151993344, LR: 0.0010000000\n",
      "Epoch: 40/500, Loss: 0.0151861769, LR: 0.0010000000\n",
      "Epoch: 41/500, Loss: 0.0151840756, LR: 0.0010000000\n",
      "Epoch: 42/500, Loss: 0.0151486835, LR: 0.0010000000\n",
      "Epoch: 43/500, Loss: 0.0151811783, LR: 0.0010000000\n",
      "Epoch: 44/500, Loss: 0.0151476361, LR: 0.0010000000\n",
      "Epoch: 45/500, Loss: 0.0151619324, LR: 0.0010000000\n",
      "Epoch: 46/500, Loss: 0.0151363060, LR: 0.0010000000\n",
      "Epoch: 47/500, Loss: 0.0151409462, LR: 0.0010000000\n",
      "Epoch: 48/500, Loss: 0.0151376020, LR: 0.0010000000\n",
      "Epoch: 49/500, Loss: 0.0151199483, LR: 0.0010000000\n",
      "Epoch: 50/500, Loss: 0.0151221453, LR: 0.0010000000\n",
      "Epoch: 51/500, Loss: 0.0150924051, LR: 0.0010000000\n",
      "Epoch: 52/500, Loss: 0.0150618502, LR: 0.0010000000\n",
      "Epoch: 53/500, Loss: 0.0151080872, LR: 0.0010000000\n",
      "Epoch: 54/500, Loss: 0.0150629866, LR: 0.0010000000\n",
      "Epoch: 55/500, Loss: 0.0150570222, LR: 0.0010000000\n",
      "Epoch: 56/500, Loss: 0.0150770442, LR: 0.0010000000\n",
      "Epoch: 57/500, Loss: 0.0150608048, LR: 0.0010000000\n",
      "Epoch: 58/500, Loss: 0.0150786986, LR: 0.0010000000\n",
      "Epoch: 59/500, Loss: 0.0150809375, LR: 0.0010000000\n",
      "Epoch: 60/500, Loss: 0.0150375374, LR: 0.0010000000\n",
      "Epoch: 61/500, Loss: 0.0150318803, LR: 0.0010000000\n",
      "Epoch: 62/500, Loss: 0.0150389728, LR: 0.0010000000\n",
      "Epoch: 63/500, Loss: 0.0150486417, LR: 0.0010000000\n",
      "Epoch: 64/500, Loss: 0.0150232032, LR: 0.0010000000\n",
      "Epoch: 65/500, Loss: 0.0150215221, LR: 0.0010000000\n",
      "Epoch: 66/500, Loss: 0.0150052028, LR: 0.0010000000\n",
      "Epoch: 67/500, Loss: 0.0150107131, LR: 0.0010000000\n",
      "Epoch: 68/500, Loss: 0.0150041616, LR: 0.0010000000\n",
      "Epoch: 69/500, Loss: 0.0150100899, LR: 0.0010000000\n",
      "Epoch: 70/500, Loss: 0.0150268735, LR: 0.0010000000\n",
      "Epoch: 71/500, Loss: 0.0149894505, LR: 0.0010000000\n",
      "Epoch: 72/500, Loss: 0.0149914565, LR: 0.0010000000\n",
      "Epoch: 73/500, Loss: 0.0149825546, LR: 0.0010000000\n",
      "Epoch: 74/500, Loss: 0.0149730844, LR: 0.0010000000\n",
      "Epoch: 75/500, Loss: 0.0149758122, LR: 0.0010000000\n",
      "Epoch: 76/500, Loss: 0.0149676204, LR: 0.0010000000\n",
      "Epoch: 77/500, Loss: 0.0149561397, LR: 0.0010000000\n",
      "Epoch: 78/500, Loss: 0.0149493392, LR: 0.0010000000\n",
      "Epoch: 79/500, Loss: 0.0149542976, LR: 0.0010000000\n",
      "Epoch: 80/500, Loss: 0.0149722177, LR: 0.0010000000\n",
      "Epoch: 81/500, Loss: 0.0149635924, LR: 0.0010000000\n",
      "Epoch: 82/500, Loss: 0.0149283908, LR: 0.0010000000\n",
      "Epoch: 83/500, Loss: 0.0149823774, LR: 0.0010000000\n",
      "Epoch: 84/500, Loss: 0.0149471581, LR: 0.0010000000\n",
      "Epoch: 85/500, Loss: 0.0149113225, LR: 0.0010000000\n",
      "Epoch: 86/500, Loss: 0.0149234714, LR: 0.0010000000\n",
      "Epoch: 87/500, Loss: 0.0149170726, LR: 0.0010000000\n",
      "Epoch: 88/500, Loss: 0.0149161051, LR: 0.0010000000\n",
      "Epoch: 89/500, Loss: 0.0149087684, LR: 0.0010000000\n",
      "Epoch: 90/500, Loss: 0.0149281135, LR: 0.0010000000\n",
      "Epoch: 91/500, Loss: 0.0149160918, LR: 0.0010000000\n",
      "Epoch: 92/500, Loss: 0.0149046411, LR: 0.0010000000\n",
      "Epoch: 93/500, Loss: 0.0148981147, LR: 0.0010000000\n",
      "Epoch: 94/500, Loss: 0.0148897146, LR: 0.0010000000\n",
      "Epoch: 95/500, Loss: 0.0149146451, LR: 0.0010000000\n",
      "Epoch: 96/500, Loss: 0.0149069420, LR: 0.0010000000\n",
      "Epoch: 97/500, Loss: 0.0148793666, LR: 0.0010000000\n",
      "Epoch: 98/500, Loss: 0.0148983502, LR: 0.0010000000\n",
      "Epoch: 99/500, Loss: 0.0148637985, LR: 0.0010000000\n",
      "Epoch: 100/500, Loss: 0.0148772302, LR: 0.0010000000\n",
      "Epoch: 101/500, Loss: 0.0148727821, LR: 0.0010000000\n",
      "Epoch: 102/500, Loss: 0.0148628449, LR: 0.0010000000\n",
      "Epoch: 103/500, Loss: 0.0148732772, LR: 0.0010000000\n",
      "Epoch: 104/500, Loss: 0.0148656578, LR: 0.0010000000\n",
      "Epoch: 105/500, Loss: 0.0148845295, LR: 0.0005000000\n",
      "Epoch: 106/500, Loss: 0.0147245651, LR: 0.0005000000\n",
      "Epoch: 107/500, Loss: 0.0146849454, LR: 0.0005000000\n",
      "Epoch: 108/500, Loss: 0.0146946087, LR: 0.0005000000\n",
      "Epoch: 109/500, Loss: 0.0146535454, LR: 0.0005000000\n",
      "Epoch: 110/500, Loss: 0.0146575533, LR: 0.0005000000\n",
      "Epoch: 111/500, Loss: 0.0146443276, LR: 0.0005000000\n",
      "Epoch: 112/500, Loss: 0.0146508677, LR: 0.0005000000\n",
      "Epoch: 113/500, Loss: 0.0146353068, LR: 0.0005000000\n",
      "Epoch: 114/500, Loss: 0.0145979228, LR: 0.0005000000\n",
      "Epoch: 115/500, Loss: 0.0146264563, LR: 0.0005000000\n",
      "Epoch: 116/500, Loss: 0.0145965817, LR: 0.0005000000\n",
      "Epoch: 117/500, Loss: 0.0146012324, LR: 0.0005000000\n",
      "Epoch: 118/500, Loss: 0.0145879962, LR: 0.0005000000\n",
      "Epoch: 119/500, Loss: 0.0145983216, LR: 0.0005000000\n",
      "Epoch: 120/500, Loss: 0.0145780104, LR: 0.0005000000\n",
      "Epoch: 121/500, Loss: 0.0145728062, LR: 0.0005000000\n",
      "Epoch: 122/500, Loss: 0.0145437191, LR: 0.0005000000\n",
      "Epoch: 123/500, Loss: 0.0145482438, LR: 0.0005000000\n",
      "Epoch: 124/500, Loss: 0.0145489442, LR: 0.0005000000\n",
      "Epoch: 125/500, Loss: 0.0145502635, LR: 0.0005000000\n",
      "Epoch: 126/500, Loss: 0.0145570316, LR: 0.0005000000\n",
      "Epoch: 127/500, Loss: 0.0145555056, LR: 0.0005000000\n",
      "Epoch: 128/500, Loss: 0.0145219434, LR: 0.0005000000\n",
      "Epoch: 129/500, Loss: 0.0145178307, LR: 0.0005000000\n",
      "Epoch: 130/500, Loss: 0.0145057734, LR: 0.0005000000\n",
      "Epoch: 131/500, Loss: 0.0145083313, LR: 0.0005000000\n",
      "Epoch: 132/500, Loss: 0.0145128769, LR: 0.0005000000\n",
      "Epoch: 133/500, Loss: 0.0144502660, LR: 0.0005000000\n",
      "Epoch: 134/500, Loss: 0.0144852977, LR: 0.0005000000\n",
      "Epoch: 135/500, Loss: 0.0144869099, LR: 0.0005000000\n",
      "Epoch: 136/500, Loss: 0.0144444425, LR: 0.0005000000\n",
      "Epoch: 137/500, Loss: 0.0144417707, LR: 0.0005000000\n",
      "Epoch: 138/500, Loss: 0.0144641838, LR: 0.0005000000\n",
      "Epoch: 139/500, Loss: 0.0144318022, LR: 0.0005000000\n",
      "Epoch: 140/500, Loss: 0.0144359283, LR: 0.0005000000\n",
      "Epoch: 141/500, Loss: 0.0144413784, LR: 0.0005000000\n",
      "Epoch: 142/500, Loss: 0.0144196602, LR: 0.0005000000\n",
      "Epoch: 143/500, Loss: 0.0144460733, LR: 0.0005000000\n",
      "Epoch: 144/500, Loss: 0.0144364732, LR: 0.0005000000\n",
      "Epoch: 145/500, Loss: 0.0143908740, LR: 0.0005000000\n",
      "Epoch: 146/500, Loss: 0.0144357433, LR: 0.0005000000\n",
      "Epoch: 147/500, Loss: 0.0144098847, LR: 0.0005000000\n",
      "Epoch: 148/500, Loss: 0.0143811840, LR: 0.0005000000\n",
      "Epoch: 149/500, Loss: 0.0143692816, LR: 0.0005000000\n",
      "Epoch: 150/500, Loss: 0.0143638781, LR: 0.0005000000\n",
      "Epoch: 151/500, Loss: 0.0143759821, LR: 0.0005000000\n",
      "Epoch: 152/500, Loss: 0.0143501282, LR: 0.0005000000\n",
      "Epoch: 153/500, Loss: 0.0143612496, LR: 0.0005000000\n",
      "Epoch: 154/500, Loss: 0.0143411695, LR: 0.0005000000\n",
      "Epoch: 155/500, Loss: 0.0143464175, LR: 0.0005000000\n",
      "Epoch: 156/500, Loss: 0.0143230708, LR: 0.0005000000\n",
      "Epoch: 157/500, Loss: 0.0143470588, LR: 0.0005000000\n",
      "Epoch: 158/500, Loss: 0.0143176136, LR: 0.0005000000\n",
      "Epoch: 159/500, Loss: 0.0143161700, LR: 0.0005000000\n",
      "Epoch: 160/500, Loss: 0.0143416481, LR: 0.0005000000\n",
      "Epoch: 161/500, Loss: 0.0143065721, LR: 0.0005000000\n",
      "Epoch: 162/500, Loss: 0.0142812969, LR: 0.0005000000\n",
      "Epoch: 163/500, Loss: 0.0142525469, LR: 0.0005000000\n",
      "Epoch: 164/500, Loss: 0.0142787465, LR: 0.0005000000\n",
      "Epoch: 165/500, Loss: 0.0142928200, LR: 0.0005000000\n",
      "Epoch: 166/500, Loss: 0.0143028268, LR: 0.0005000000\n",
      "Epoch: 167/500, Loss: 0.0143165034, LR: 0.0005000000\n",
      "Epoch: 168/500, Loss: 0.0142962242, LR: 0.0005000000\n",
      "Epoch: 169/500, Loss: 0.0142599276, LR: 0.0002500000\n",
      "Epoch: 170/500, Loss: 0.0141055516, LR: 0.0002500000\n",
      "Epoch: 171/500, Loss: 0.0140444640, LR: 0.0002500000\n",
      "Epoch: 172/500, Loss: 0.0140695349, LR: 0.0002500000\n",
      "Epoch: 173/500, Loss: 0.0140300418, LR: 0.0002500000\n",
      "Epoch: 174/500, Loss: 0.0140444303, LR: 0.0002500000\n",
      "Epoch: 175/500, Loss: 0.0140042614, LR: 0.0002500000\n",
      "Epoch: 176/500, Loss: 0.0140148410, LR: 0.0002500000\n",
      "Epoch: 177/500, Loss: 0.0139933671, LR: 0.0002500000\n",
      "Epoch: 178/500, Loss: 0.0139783086, LR: 0.0002500000\n",
      "Epoch: 179/500, Loss: 0.0140141396, LR: 0.0002500000\n",
      "Epoch: 180/500, Loss: 0.0139895836, LR: 0.0002500000\n",
      "Epoch: 181/500, Loss: 0.0139800689, LR: 0.0002500000\n",
      "Epoch: 182/500, Loss: 0.0139662434, LR: 0.0002500000\n",
      "Epoch: 183/500, Loss: 0.0139708779, LR: 0.0002500000\n",
      "Epoch: 184/500, Loss: 0.0139323682, LR: 0.0002500000\n",
      "Epoch: 185/500, Loss: 0.0139421347, LR: 0.0002500000\n",
      "Epoch: 186/500, Loss: 0.0139781608, LR: 0.0002500000\n",
      "Epoch: 187/500, Loss: 0.0139295076, LR: 0.0002500000\n",
      "Epoch: 188/500, Loss: 0.0139473307, LR: 0.0002500000\n",
      "Epoch: 189/500, Loss: 0.0139017110, LR: 0.0002500000\n",
      "Epoch: 190/500, Loss: 0.0139164532, LR: 0.0002500000\n",
      "Epoch: 191/500, Loss: 0.0139492271, LR: 0.0002500000\n",
      "Epoch: 192/500, Loss: 0.0139126773, LR: 0.0002500000\n",
      "Epoch: 193/500, Loss: 0.0138928307, LR: 0.0002500000\n",
      "Epoch: 194/500, Loss: 0.0138899149, LR: 0.0002500000\n",
      "Epoch: 195/500, Loss: 0.0139132824, LR: 0.0002500000\n",
      "Epoch: 196/500, Loss: 0.0138835249, LR: 0.0002500000\n",
      "Epoch: 197/500, Loss: 0.0139247559, LR: 0.0002500000\n",
      "Epoch: 198/500, Loss: 0.0138693148, LR: 0.0002500000\n",
      "Epoch: 199/500, Loss: 0.0138789438, LR: 0.0002500000\n",
      "Epoch: 200/500, Loss: 0.0138890869, LR: 0.0002500000\n",
      "Epoch: 201/500, Loss: 0.0138750892, LR: 0.0002500000\n",
      "Epoch: 202/500, Loss: 0.0138495477, LR: 0.0002500000\n",
      "Epoch: 203/500, Loss: 0.0138495243, LR: 0.0002500000\n",
      "Epoch: 204/500, Loss: 0.0138710498, LR: 0.0002500000\n",
      "Epoch: 205/500, Loss: 0.0138326305, LR: 0.0002500000\n",
      "Epoch: 206/500, Loss: 0.0138379547, LR: 0.0002500000\n",
      "Epoch: 207/500, Loss: 0.0138229329, LR: 0.0002500000\n",
      "Epoch: 208/500, Loss: 0.0138449247, LR: 0.0002500000\n",
      "Epoch: 209/500, Loss: 0.0138061602, LR: 0.0002500000\n",
      "Epoch: 210/500, Loss: 0.0138056280, LR: 0.0002500000\n",
      "Epoch: 211/500, Loss: 0.0137991302, LR: 0.0002500000\n",
      "Epoch: 212/500, Loss: 0.0138388289, LR: 0.0002500000\n",
      "Epoch: 213/500, Loss: 0.0137860312, LR: 0.0002500000\n",
      "Epoch: 214/500, Loss: 0.0137978789, LR: 0.0002500000\n",
      "Epoch: 215/500, Loss: 0.0137735618, LR: 0.0002500000\n",
      "Epoch: 216/500, Loss: 0.0138099338, LR: 0.0002500000\n",
      "Epoch: 217/500, Loss: 0.0137836997, LR: 0.0002500000\n",
      "Epoch: 218/500, Loss: 0.0138016973, LR: 0.0002500000\n",
      "Epoch: 219/500, Loss: 0.0137639035, LR: 0.0002500000\n",
      "Epoch: 220/500, Loss: 0.0137509250, LR: 0.0002500000\n",
      "Epoch: 221/500, Loss: 0.0137561678, LR: 0.0002500000\n",
      "Epoch: 222/500, Loss: 0.0137392971, LR: 0.0002500000\n",
      "Epoch: 223/500, Loss: 0.0137901881, LR: 0.0002500000\n",
      "Epoch: 224/500, Loss: 0.0137376929, LR: 0.0002500000\n",
      "Epoch: 225/500, Loss: 0.0137574753, LR: 0.0002500000\n",
      "Epoch: 226/500, Loss: 0.0137824429, LR: 0.0002500000\n",
      "Epoch: 227/500, Loss: 0.0137291157, LR: 0.0002500000\n",
      "Epoch: 228/500, Loss: 0.0137212756, LR: 0.0002500000\n",
      "Epoch: 229/500, Loss: 0.0137077978, LR: 0.0002500000\n",
      "Epoch: 230/500, Loss: 0.0136932853, LR: 0.0002500000\n",
      "Epoch: 231/500, Loss: 0.0137198682, LR: 0.0002500000\n",
      "Epoch: 232/500, Loss: 0.0137194086, LR: 0.0002500000\n",
      "Epoch: 233/500, Loss: 0.0137465009, LR: 0.0002500000\n",
      "Epoch: 234/500, Loss: 0.0137203919, LR: 0.0002500000\n",
      "Epoch: 235/500, Loss: 0.0137050335, LR: 0.0002500000\n",
      "Epoch: 236/500, Loss: 0.0136850050, LR: 0.0002500000\n",
      "Epoch: 237/500, Loss: 0.0137212033, LR: 0.0002500000\n",
      "Epoch: 238/500, Loss: 0.0136809447, LR: 0.0002500000\n",
      "Epoch: 239/500, Loss: 0.0137014278, LR: 0.0002500000\n",
      "Epoch: 240/500, Loss: 0.0137135570, LR: 0.0002500000\n",
      "Epoch: 241/500, Loss: 0.0137278624, LR: 0.0002500000\n",
      "Epoch: 242/500, Loss: 0.0136805008, LR: 0.0002500000\n",
      "Epoch: 243/500, Loss: 0.0136796002, LR: 0.0002500000\n",
      "Epoch: 244/500, Loss: 0.0137097543, LR: 0.0001250000\n",
      "Epoch: 245/500, Loss: 0.0135471495, LR: 0.0001250000\n",
      "Epoch: 246/500, Loss: 0.0135612591, LR: 0.0001250000\n",
      "Epoch: 247/500, Loss: 0.0135041371, LR: 0.0001250000\n",
      "Epoch: 248/500, Loss: 0.0134988788, LR: 0.0001250000\n",
      "Epoch: 249/500, Loss: 0.0134996571, LR: 0.0001250000\n",
      "Epoch: 250/500, Loss: 0.0135001015, LR: 0.0001250000\n",
      "Epoch: 251/500, Loss: 0.0135213393, LR: 0.0001250000\n",
      "Epoch: 252/500, Loss: 0.0134706222, LR: 0.0001250000\n",
      "Epoch: 253/500, Loss: 0.0134993691, LR: 0.0001250000\n",
      "Epoch: 254/500, Loss: 0.0134437739, LR: 0.0001250000\n",
      "Epoch: 255/500, Loss: 0.0134675955, LR: 0.0001250000\n",
      "Epoch: 256/500, Loss: 0.0134710082, LR: 0.0001250000\n",
      "Epoch: 257/500, Loss: 0.0134466312, LR: 0.0001250000\n",
      "Epoch: 258/500, Loss: 0.0134425027, LR: 0.0001250000\n",
      "Epoch: 259/500, Loss: 0.0134862562, LR: 0.0001250000\n",
      "Epoch: 260/500, Loss: 0.0134756628, LR: 0.0000625000\n",
      "Epoch: 261/500, Loss: 0.0134154203, LR: 0.0000625000\n",
      "Epoch: 262/500, Loss: 0.0134024453, LR: 0.0000625000\n",
      "Epoch: 263/500, Loss: 0.0133837694, LR: 0.0000625000\n",
      "Epoch: 264/500, Loss: 0.0133514933, LR: 0.0000625000\n",
      "Epoch: 265/500, Loss: 0.0133706012, LR: 0.0000625000\n",
      "Epoch: 266/500, Loss: 0.0133827909, LR: 0.0000625000\n",
      "Epoch: 267/500, Loss: 0.0133843978, LR: 0.0000625000\n",
      "Epoch: 268/500, Loss: 0.0133868132, LR: 0.0000625000\n",
      "Epoch: 269/500, Loss: 0.0133492585, LR: 0.0000625000\n",
      "Epoch: 270/500, Loss: 0.0133335524, LR: 0.0000625000\n",
      "Epoch: 271/500, Loss: 0.0133535526, LR: 0.0000625000\n",
      "Epoch: 272/500, Loss: 0.0133692875, LR: 0.0000625000\n",
      "Epoch: 273/500, Loss: 0.0133413641, LR: 0.0000625000\n",
      "Epoch: 274/500, Loss: 0.0133542396, LR: 0.0000625000\n",
      "Epoch: 275/500, Loss: 0.0133288181, LR: 0.0000625000\n",
      "Epoch: 276/500, Loss: 0.0133346149, LR: 0.0000625000\n",
      "Epoch: 277/500, Loss: 0.0133748383, LR: 0.0000625000\n",
      "Epoch: 278/500, Loss: 0.0133846471, LR: 0.0000625000\n",
      "Epoch: 279/500, Loss: 0.0133556259, LR: 0.0000625000\n",
      "Epoch: 280/500, Loss: 0.0133654193, LR: 0.0000625000\n",
      "Epoch: 281/500, Loss: 0.0133489836, LR: 0.0000312500\n",
      "Epoch: 282/500, Loss: 0.0133013390, LR: 0.0000312500\n",
      "Epoch: 283/500, Loss: 0.0133156400, LR: 0.0000312500\n",
      "Epoch: 284/500, Loss: 0.0132908342, LR: 0.0000312500\n",
      "Epoch: 285/500, Loss: 0.0132803007, LR: 0.0000312500\n",
      "Epoch: 286/500, Loss: 0.0133229088, LR: 0.0000312500\n",
      "Epoch: 287/500, Loss: 0.0132910655, LR: 0.0000312500\n",
      "Epoch: 288/500, Loss: 0.0132900939, LR: 0.0000312500\n",
      "Epoch: 289/500, Loss: 0.0133090891, LR: 0.0000312500\n",
      "Epoch: 290/500, Loss: 0.0132561789, LR: 0.0000312500\n",
      "Epoch: 291/500, Loss: 0.0133139316, LR: 0.0000312500\n",
      "Epoch: 292/500, Loss: 0.0133120061, LR: 0.0000312500\n",
      "Epoch: 293/500, Loss: 0.0132676107, LR: 0.0000312500\n",
      "Epoch: 294/500, Loss: 0.0132600319, LR: 0.0000312500\n",
      "Epoch: 295/500, Loss: 0.0132655252, LR: 0.0000312500\n",
      "Epoch: 296/500, Loss: 0.0132468113, LR: 0.0000312500\n",
      "Epoch: 297/500, Loss: 0.0133165611, LR: 0.0000312500\n",
      "Epoch: 298/500, Loss: 0.0132400330, LR: 0.0000312500\n",
      "Epoch: 299/500, Loss: 0.0132943333, LR: 0.0000312500\n",
      "Epoch: 300/500, Loss: 0.0132620915, LR: 0.0000312500\n",
      "Epoch: 301/500, Loss: 0.0132906569, LR: 0.0000312500\n",
      "Epoch: 302/500, Loss: 0.0132866094, LR: 0.0000312500\n",
      "Epoch: 303/500, Loss: 0.0132634842, LR: 0.0000312500\n",
      "Epoch: 304/500, Loss: 0.0132844648, LR: 0.0000156250\n",
      "Epoch: 305/500, Loss: 0.0132458739, LR: 0.0000156250\n",
      "Epoch: 306/500, Loss: 0.0132744927, LR: 0.0000156250\n",
      "Epoch: 307/500, Loss: 0.0132455679, LR: 0.0000156250\n",
      "Epoch: 308/500, Loss: 0.0132423460, LR: 0.0000156250\n",
      "Epoch: 309/500, Loss: 0.0132342970, LR: 0.0000156250\n",
      "Epoch: 310/500, Loss: 0.0132516465, LR: 0.0000156250\n",
      "Epoch: 311/500, Loss: 0.0132722475, LR: 0.0000156250\n",
      "Epoch: 312/500, Loss: 0.0131995744, LR: 0.0000156250\n",
      "Epoch: 313/500, Loss: 0.0132339076, LR: 0.0000156250\n",
      "Epoch: 314/500, Loss: 0.0132427182, LR: 0.0000156250\n",
      "Epoch: 315/500, Loss: 0.0132290864, LR: 0.0000156250\n",
      "Epoch: 316/500, Loss: 0.0132522940, LR: 0.0000156250\n",
      "Epoch: 317/500, Loss: 0.0132794565, LR: 0.0000156250\n",
      "Epoch: 318/500, Loss: 0.0132432640, LR: 0.0000078125\n",
      "Epoch: 319/500, Loss: 0.0132710131, LR: 0.0000078125\n",
      "Epoch: 320/500, Loss: 0.0132627225, LR: 0.0000078125\n",
      "Epoch: 321/500, Loss: 0.0132461608, LR: 0.0000078125\n",
      "Epoch: 322/500, Loss: 0.0132332506, LR: 0.0000078125\n",
      "Epoch: 323/500, Loss: 0.0132297072, LR: 0.0000078125\n",
      "Epoch: 324/500, Loss: 0.0132213976, LR: 0.0000039063\n",
      "Epoch: 325/500, Loss: 0.0132181666, LR: 0.0000039063\n",
      "Epoch: 326/500, Loss: 0.0132295650, LR: 0.0000039063\n",
      "Epoch: 327/500, Loss: 0.0132297471, LR: 0.0000039063\n",
      "Epoch: 328/500, Loss: 0.0132394824, LR: 0.0000039063\n",
      "Epoch: 329/500, Loss: 0.0132149283, LR: 0.0000039063\n",
      "Epoch: 330/500, Loss: 0.0132424870, LR: 0.0000019531\n",
      "Epoch: 331/500, Loss: 0.0132147302, LR: 0.0000019531\n",
      "Epoch: 332/500, Loss: 0.0132310091, LR: 0.0000019531\n",
      "Epoch: 333/500, Loss: 0.0132319075, LR: 0.0000019531\n",
      "Epoch: 334/500, Loss: 0.0132193470, LR: 0.0000019531\n",
      "Epoch: 335/500, Loss: 0.0131936536, LR: 0.0000019531\n",
      "Epoch: 336/500, Loss: 0.0132109365, LR: 0.0000019531\n",
      "Epoch: 337/500, Loss: 0.0132376097, LR: 0.0000019531\n",
      "Epoch: 338/500, Loss: 0.0132175701, LR: 0.0000019531\n",
      "Epoch: 339/500, Loss: 0.0131963367, LR: 0.0000019531\n",
      "Epoch: 340/500, Loss: 0.0132335937, LR: 0.0000019531\n",
      "Epoch: 341/500, Loss: 0.0132213614, LR: 0.0000009766\n",
      "Epoch: 342/500, Loss: 0.0132466297, LR: 0.0000009766\n",
      "Epoch: 343/500, Loss: 0.0132506967, LR: 0.0000009766\n",
      "Epoch: 344/500, Loss: 0.0132137915, LR: 0.0000009766\n",
      "Epoch: 345/500, Loss: 0.0132536073, LR: 0.0000009766\n",
      "Epoch: 346/500, Loss: 0.0132143310, LR: 0.0000009766\n",
      "Epoch: 347/500, Loss: 0.0132314998, LR: 0.0000004883\n",
      "Epoch: 348/500, Loss: 0.0132199758, LR: 0.0000004883\n",
      "Epoch: 349/500, Loss: 0.0132076100, LR: 0.0000004883\n",
      "Epoch: 350/500, Loss: 0.0132359878, LR: 0.0000004883\n",
      "Epoch: 351/500, Loss: 0.0131991612, LR: 0.0000004883\n",
      "Epoch: 352/500, Loss: 0.0132062849, LR: 0.0000004883\n",
      "Epoch: 353/500, Loss: 0.0132517956, LR: 0.0000002441\n",
      "Epoch: 354/500, Loss: 0.0132354009, LR: 0.0000002441\n",
      "Epoch: 355/500, Loss: 0.0132228812, LR: 0.0000002441\n",
      "Epoch: 356/500, Loss: 0.0132019740, LR: 0.0000002441\n",
      "Epoch: 357/500, Loss: 0.0132055498, LR: 0.0000002441\n",
      "Epoch: 358/500, Loss: 0.0132309928, LR: 0.0000002441\n",
      "Epoch: 359/500, Loss: 0.0132113887, LR: 0.0000001221\n",
      "Epoch: 360/500, Loss: 0.0132283929, LR: 0.0000001221\n",
      "Epoch: 361/500, Loss: 0.0132120965, LR: 0.0000001221\n",
      "Epoch: 362/500, Loss: 0.0132423119, LR: 0.0000001221\n",
      "Epoch: 363/500, Loss: 0.0132217667, LR: 0.0000001221\n",
      "Epoch: 364/500, Loss: 0.0132089508, LR: 0.0000001221\n",
      "Epoch: 365/500, Loss: 0.0132433367, LR: 0.0000000610\n",
      "Epoch: 366/500, Loss: 0.0132239282, LR: 0.0000000610\n",
      "Epoch: 367/500, Loss: 0.0132192198, LR: 0.0000000610\n",
      "Epoch: 368/500, Loss: 0.0132035409, LR: 0.0000000610\n",
      "Epoch: 369/500, Loss: 0.0131995582, LR: 0.0000000610\n",
      "Epoch: 370/500, Loss: 0.0131971688, LR: 0.0000000610\n",
      "Epoch: 371/500, Loss: 0.0132054357, LR: 0.0000000305\n",
      "Epoch: 372/500, Loss: 0.0132317172, LR: 0.0000000305\n",
      "Epoch: 373/500, Loss: 0.0132028283, LR: 0.0000000305\n",
      "Epoch: 374/500, Loss: 0.0132255807, LR: 0.0000000305\n",
      "Epoch: 375/500, Loss: 0.0132113081, LR: 0.0000000305\n",
      "Epoch: 376/500, Loss: 0.0132116841, LR: 0.0000000305\n",
      "Epoch: 377/500, Loss: 0.0132212826, LR: 0.0000000153\n",
      "Epoch: 378/500, Loss: 0.0132168448, LR: 0.0000000153\n",
      "Epoch: 379/500, Loss: 0.0132409167, LR: 0.0000000153\n",
      "Epoch: 380/500, Loss: 0.0132219126, LR: 0.0000000153\n",
      "Epoch: 381/500, Loss: 0.0131932678, LR: 0.0000000153\n",
      "Epoch: 382/500, Loss: 0.0132343193, LR: 0.0000000153\n",
      "Epoch: 383/500, Loss: 0.0132191994, LR: 0.0000000153\n",
      "Epoch: 384/500, Loss: 0.0132391084, LR: 0.0000000153\n",
      "Epoch: 385/500, Loss: 0.0132206548, LR: 0.0000000153\n",
      "Epoch: 386/500, Loss: 0.0132631803, LR: 0.0000000153\n",
      "Epoch: 387/500, Loss: 0.0132013371, LR: 0.0000000153\n",
      "Epoch: 388/500, Loss: 0.0132395910, LR: 0.0000000153\n",
      "Epoch: 389/500, Loss: 0.0132626095, LR: 0.0000000153\n",
      "Epoch: 390/500, Loss: 0.0132032501, LR: 0.0000000153\n",
      "Epoch: 391/500, Loss: 0.0131673337, LR: 0.0000000153\n",
      "Epoch: 392/500, Loss: 0.0132001189, LR: 0.0000000153\n",
      "Epoch: 393/500, Loss: 0.0132392661, LR: 0.0000000153\n",
      "Epoch: 394/500, Loss: 0.0132372152, LR: 0.0000000153\n",
      "Epoch: 395/500, Loss: 0.0131859735, LR: 0.0000000153\n",
      "Epoch: 396/500, Loss: 0.0132015423, LR: 0.0000000153\n",
      "Epoch: 397/500, Loss: 0.0131940056, LR: 0.0000000153\n",
      "Epoch: 398/500, Loss: 0.0132386459, LR: 0.0000000153\n",
      "Epoch: 399/500, Loss: 0.0131971368, LR: 0.0000000153\n",
      "Epoch: 400/500, Loss: 0.0132054547, LR: 0.0000000153\n",
      "Epoch: 401/500, Loss: 0.0131984717, LR: 0.0000000153\n",
      "Epoch: 402/500, Loss: 0.0132009715, LR: 0.0000000153\n",
      "Epoch: 403/500, Loss: 0.0132312837, LR: 0.0000000153\n",
      "Epoch: 404/500, Loss: 0.0132204280, LR: 0.0000000153\n",
      "Epoch: 405/500, Loss: 0.0131966128, LR: 0.0000000153\n",
      "Epoch: 406/500, Loss: 0.0132300496, LR: 0.0000000153\n",
      "Epoch: 407/500, Loss: 0.0132242450, LR: 0.0000000153\n",
      "Epoch: 408/500, Loss: 0.0132086031, LR: 0.0000000153\n",
      "Epoch: 409/500, Loss: 0.0132143541, LR: 0.0000000153\n",
      "Epoch: 410/500, Loss: 0.0132334645, LR: 0.0000000153\n",
      "Epoch: 411/500, Loss: 0.0132329154, LR: 0.0000000153\n",
      "Epoch: 412/500, Loss: 0.0131916593, LR: 0.0000000153\n",
      "Epoch: 413/500, Loss: 0.0132144268, LR: 0.0000000153\n",
      "Epoch: 414/500, Loss: 0.0131943548, LR: 0.0000000153\n",
      "Epoch: 415/500, Loss: 0.0132413673, LR: 0.0000000153\n",
      "Epoch: 416/500, Loss: 0.0132037478, LR: 0.0000000153\n",
      "Epoch: 417/500, Loss: 0.0132423084, LR: 0.0000000153\n",
      "Epoch: 418/500, Loss: 0.0132327786, LR: 0.0000000153\n",
      "Epoch: 419/500, Loss: 0.0132010826, LR: 0.0000000153\n",
      "Epoch: 420/500, Loss: 0.0132226387, LR: 0.0000000153\n",
      "Epoch: 421/500, Loss: 0.0132448346, LR: 0.0000000153\n",
      "Epoch: 422/500, Loss: 0.0132187497, LR: 0.0000000153\n",
      "Epoch: 423/500, Loss: 0.0132113346, LR: 0.0000000153\n",
      "Epoch: 424/500, Loss: 0.0132209202, LR: 0.0000000153\n",
      "Epoch: 425/500, Loss: 0.0132325142, LR: 0.0000000153\n",
      "Epoch: 426/500, Loss: 0.0132221114, LR: 0.0000000153\n",
      "Epoch: 427/500, Loss: 0.0132214055, LR: 0.0000000153\n",
      "Epoch: 428/500, Loss: 0.0131940891, LR: 0.0000000153\n",
      "Epoch: 429/500, Loss: 0.0131632139, LR: 0.0000000153\n",
      "Epoch: 430/500, Loss: 0.0131722464, LR: 0.0000000153\n",
      "Epoch: 431/500, Loss: 0.0131949622, LR: 0.0000000153\n",
      "Epoch: 432/500, Loss: 0.0131716183, LR: 0.0000000153\n",
      "Epoch: 433/500, Loss: 0.0132036459, LR: 0.0000000153\n",
      "Epoch: 434/500, Loss: 0.0132079775, LR: 0.0000000153\n",
      "Epoch: 435/500, Loss: 0.0132213850, LR: 0.0000000153\n",
      "Epoch: 436/500, Loss: 0.0132008422, LR: 0.0000000153\n",
      "Epoch: 437/500, Loss: 0.0132521475, LR: 0.0000000153\n",
      "Epoch: 438/500, Loss: 0.0132332677, LR: 0.0000000153\n",
      "Epoch: 439/500, Loss: 0.0131983518, LR: 0.0000000153\n",
      "Epoch: 440/500, Loss: 0.0131879485, LR: 0.0000000153\n",
      "Epoch: 441/500, Loss: 0.0132122103, LR: 0.0000000153\n",
      "Epoch: 442/500, Loss: 0.0131864435, LR: 0.0000000153\n",
      "Epoch: 443/500, Loss: 0.0132060111, LR: 0.0000000153\n",
      "Epoch: 444/500, Loss: 0.0132040385, LR: 0.0000000153\n",
      "Epoch: 445/500, Loss: 0.0131984095, LR: 0.0000000153\n",
      "Epoch: 446/500, Loss: 0.0132321546, LR: 0.0000000153\n",
      "Epoch: 447/500, Loss: 0.0131913240, LR: 0.0000000153\n",
      "Epoch: 448/500, Loss: 0.0132259367, LR: 0.0000000153\n",
      "Epoch: 449/500, Loss: 0.0132540355, LR: 0.0000000153\n",
      "Epoch: 450/500, Loss: 0.0131891397, LR: 0.0000000153\n",
      "Epoch: 451/500, Loss: 0.0132208264, LR: 0.0000000153\n",
      "Epoch: 452/500, Loss: 0.0132129110, LR: 0.0000000153\n",
      "Epoch: 453/500, Loss: 0.0132203340, LR: 0.0000000153\n",
      "Epoch: 454/500, Loss: 0.0132368723, LR: 0.0000000153\n",
      "Epoch: 455/500, Loss: 0.0132623244, LR: 0.0000000153\n",
      "Epoch: 456/500, Loss: 0.0132028534, LR: 0.0000000153\n",
      "Epoch: 457/500, Loss: 0.0132117269, LR: 0.0000000153\n",
      "Epoch: 458/500, Loss: 0.0131805213, LR: 0.0000000153\n",
      "Epoch: 459/500, Loss: 0.0132186229, LR: 0.0000000153\n",
      "Epoch: 460/500, Loss: 0.0132177272, LR: 0.0000000153\n",
      "Epoch: 461/500, Loss: 0.0132113376, LR: 0.0000000153\n",
      "Epoch: 462/500, Loss: 0.0132368059, LR: 0.0000000153\n",
      "Epoch: 463/500, Loss: 0.0131886280, LR: 0.0000000153\n",
      "Epoch: 464/500, Loss: 0.0131968581, LR: 0.0000000153\n",
      "Epoch: 465/500, Loss: 0.0131933993, LR: 0.0000000153\n",
      "Epoch: 466/500, Loss: 0.0132249953, LR: 0.0000000153\n",
      "Epoch: 467/500, Loss: 0.0132210119, LR: 0.0000000153\n",
      "Epoch: 468/500, Loss: 0.0132288980, LR: 0.0000000153\n",
      "Epoch: 469/500, Loss: 0.0131991322, LR: 0.0000000153\n",
      "Epoch: 470/500, Loss: 0.0132223287, LR: 0.0000000153\n",
      "Epoch: 471/500, Loss: 0.0132157875, LR: 0.0000000153\n",
      "Epoch: 472/500, Loss: 0.0132159974, LR: 0.0000000153\n",
      "Epoch: 473/500, Loss: 0.0132089865, LR: 0.0000000153\n",
      "Epoch: 474/500, Loss: 0.0132079247, LR: 0.0000000153\n",
      "Epoch: 475/500, Loss: 0.0131976860, LR: 0.0000000153\n",
      "Epoch: 476/500, Loss: 0.0132024037, LR: 0.0000000153\n",
      "Epoch: 477/500, Loss: 0.0131629490, LR: 0.0000000153\n",
      "Epoch: 478/500, Loss: 0.0132004269, LR: 0.0000000153\n",
      "Epoch: 479/500, Loss: 0.0132054756, LR: 0.0000000153\n",
      "Epoch: 480/500, Loss: 0.0132028008, LR: 0.0000000153\n",
      "Epoch: 481/500, Loss: 0.0132312750, LR: 0.0000000153\n",
      "Epoch: 482/500, Loss: 0.0131990763, LR: 0.0000000153\n",
      "Epoch: 483/500, Loss: 0.0132065124, LR: 0.0000000153\n",
      "Epoch: 484/500, Loss: 0.0132415136, LR: 0.0000000153\n",
      "Epoch: 485/500, Loss: 0.0132201717, LR: 0.0000000153\n",
      "Epoch: 486/500, Loss: 0.0132230843, LR: 0.0000000153\n",
      "Epoch: 487/500, Loss: 0.0131910397, LR: 0.0000000153\n",
      "Epoch: 488/500, Loss: 0.0132086256, LR: 0.0000000153\n",
      "Epoch: 489/500, Loss: 0.0132075280, LR: 0.0000000153\n",
      "Epoch: 490/500, Loss: 0.0132078964, LR: 0.0000000153\n",
      "Epoch: 491/500, Loss: 0.0132138257, LR: 0.0000000153\n",
      "Epoch: 492/500, Loss: 0.0131744210, LR: 0.0000000153\n",
      "Epoch: 493/500, Loss: 0.0132151788, LR: 0.0000000153\n",
      "Epoch: 494/500, Loss: 0.0131962757, LR: 0.0000000153\n",
      "Epoch: 495/500, Loss: 0.0132397813, LR: 0.0000000153\n",
      "Epoch: 496/500, Loss: 0.0132292685, LR: 0.0000000153\n",
      "Epoch: 497/500, Loss: 0.0132335285, LR: 0.0000000153\n",
      "Epoch: 498/500, Loss: 0.0132553975, LR: 0.0000000153\n",
      "Epoch: 499/500, Loss: 0.0132020349, LR: 0.0000000153\n",
      "Epoch: 500/500, Loss: 0.0132433783, LR: 0.0000000153\n"
     ]
    }
   ],
   "source": [
    "model = CursorRNN(input_size, hidden_size, output_size, num_layers=lstm_layers).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()  # loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # optimizer\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    ")  # learning rate scheduler for better convergence\n",
    "\n",
    "# for best model tracking\n",
    "best_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(\n",
    "            device\n",
    "        )  # moving batch to gpu or cpu based on availability\n",
    "        optimizer.zero_grad()  # zeroing the gradients\n",
    "\n",
    "        y_pred, _ = model(x_batch)  # forward pass\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)  # calculating loss\n",
    "        loss.backward()  # backpropagation\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), max_norm=1.0\n",
    "        )  # gradient clipping to prevent exploding gradients\n",
    "\n",
    "        optimizer.step()  # updating weights\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    scheduler.step(avg_loss)  # updating learning rate based on loss\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "\n",
    "    print(\n",
    "        f'Epoch: {epoch+1}/{epochs}, Loss: {avg_loss:.10f}, LR: {optimizer.param_groups[0][\"lr\"]:.10f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at /Users/arpanbhandari/Documents/coding/bumblebee/data/models/cursor-rnn-model-2025-03-18-18:56:50.pth\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"cursor-rnn-model-{time.strftime('%Y-%m-%d-%H:%M:%S')}.pth\"\n",
    "model_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"models\", model_name)\n",
    "torch.save(best_model_state, model_path)\n",
    "print(f\"Model saved at {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bumblebee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
