{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Training Notebook  \n",
    "\n",
    "This notebook trains an AI model to predict mouse cursor movement paths. The model is built using a Recurrent Neural Network (RNN) with an LSTM layer for attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(\n",
    "    os.path.dirname(os.getcwd()),\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"cleaned-data-39-steps-merged-prepared-data-2025-03-08-16:27:23.json\",\n",
    ")  # this path is for the cleaned data; must be changed accordingly\n",
    "\n",
    "print(\n",
    "    f\"{dataset_path} exists: {\"Yes\" if os.path.exists(dataset_path) else 'No'}\"\n",
    ")  ## must be Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = open(dataset_path, \"r\")\n",
    "dataset_json = json.load(dataset_file)\n",
    "dataset_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_coordinate = 0\n",
    "max_coordinate = 1920  # the maximum display resolution used is 1920x1080 so taking 1920 as the max coordinate\n",
    "\n",
    "\n",
    "def normalize(data):  # Normalizing the data to be in the range [0, 1]\n",
    "    return (data - min_coordinate) / (max_coordinate - min_coordinate)\n",
    "\n",
    "\n",
    "def denormalize(\n",
    "    data,\n",
    "):  # Denormalizing the data to convert it back to the original range\n",
    "    return (data * (max_coordinate - min_coordinate)) + min_coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array(dataset_json[\"input\"], dtype=np.float32)\n",
    "output_data = np.array(dataset_json[\"output\"], dtype=np.float32)\n",
    "\n",
    "input_data = normalize(input_data)\n",
    "output_data = normalize(output_data)\n",
    "\n",
    "intermediate_steps_num = output_data.shape[1]\n",
    "\n",
    "X_tensor = torch.tensor(input_data, dtype=torch.float, device=device)\n",
    "y_tensor = torch.tensor(output_data, dtype=torch.float, device=device)\n",
    "\n",
    "X_tensor = X_tensor.unsqueeze(1)\n",
    "y_tensor = y_tensor = y_tensor.view(\n",
    "    -1, 2 * intermediate_steps_num\n",
    ")  # Flattening the output tensor, 2 is used because only x, y corrdinates are needed to be predicted for each step\n",
    "\n",
    "\n",
    "del input_data, output_data  # to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(\n",
    "            hidden_dim, 1, bias=False\n",
    "        )  # Attention layer to assign weights to different time steps\n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        scores = self.attn(lstm_out)  # Compute attention scores for each time step\n",
    "        attn_weights = torch.softmax(\n",
    "            scores, dim=1\n",
    "        )  # Apply softmax to normalize attention weights\n",
    "        context = torch.sum(\n",
    "            attn_weights * lstm_out, dim=1\n",
    "        )  # Create context vector by weighted sum of LSTM outputs\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class CursorRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.2):\n",
    "        super(CursorRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=dropout,\n",
    "        )  # LSTM layer for sequence processing\n",
    "        self.attention = Attention(\n",
    "            hidden_dim\n",
    "        )  # Attention mechanism to focus on important time steps\n",
    "        self.residual_fc = nn.Linear(\n",
    "            input_dim, hidden_dim\n",
    "        )  # Residual connection to help with gradient flow\n",
    "        self.layer_norm = nn.LayerNorm(\n",
    "            hidden_dim\n",
    "        )  # Layer normalization for training stability\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_dim, output_dim\n",
    "        )  # Output projection layer to generate final predictions\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # Process sequence through LSTM\n",
    "        context, attn_weights = self.attention(\n",
    "            lstm_out\n",
    "        )  # Apply attention to focus on relevant parts\n",
    "        residual = self.residual_fc(\n",
    "            x[:, -1, :]\n",
    "        )  # Create residual connection from last input\n",
    "        combined = self.layer_norm(\n",
    "            context + residual\n",
    "        )  # Combine attention output with residual and normalize\n",
    "        output = self.fc(combined)  # Generate final trajectory prediction\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_tensor.shape[2]\n",
    "output_size = y_tensor.shape[1]\n",
    "\n",
    "hidden_size = (input_size**2) * int(output_size ** (1 / 2)) * 4\n",
    "epochs = 500\n",
    "lstm_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, num_workers=0, pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500, Loss: 0.0069492316, LR: 0.0010000000\n",
      "Epoch: 2/500, Loss: 0.0039896650, LR: 0.0010000000\n",
      "Epoch: 3/500, Loss: 0.0040888302, LR: 0.0010000000\n",
      "Epoch: 4/500, Loss: 0.0040622482, LR: 0.0010000000\n",
      "Epoch: 5/500, Loss: 0.0040255222, LR: 0.0010000000\n",
      "Epoch: 6/500, Loss: 0.0040010492, LR: 0.0010000000\n",
      "Epoch: 7/500, Loss: 0.0039247756, LR: 0.0010000000\n",
      "Epoch: 8/500, Loss: 0.0039617283, LR: 0.0010000000\n",
      "Epoch: 9/500, Loss: 0.0039001164, LR: 0.0010000000\n",
      "Epoch: 10/500, Loss: 0.0038046737, LR: 0.0010000000\n",
      "Epoch: 11/500, Loss: 0.0037908000, LR: 0.0010000000\n",
      "Epoch: 12/500, Loss: 0.0037980298, LR: 0.0010000000\n",
      "Epoch: 13/500, Loss: 0.0037094286, LR: 0.0010000000\n",
      "Epoch: 14/500, Loss: 0.0037699338, LR: 0.0010000000\n",
      "Epoch: 15/500, Loss: 0.0037692718, LR: 0.0010000000\n",
      "Epoch: 16/500, Loss: 0.0037420403, LR: 0.0010000000\n",
      "Epoch: 17/500, Loss: 0.0036581326, LR: 0.0010000000\n",
      "Epoch: 18/500, Loss: 0.0036941812, LR: 0.0010000000\n",
      "Epoch: 19/500, Loss: 0.0036428810, LR: 0.0010000000\n",
      "Epoch: 20/500, Loss: 0.0036286195, LR: 0.0010000000\n",
      "Epoch: 21/500, Loss: 0.0036028735, LR: 0.0010000000\n",
      "Epoch: 22/500, Loss: 0.0035661431, LR: 0.0010000000\n",
      "Epoch: 23/500, Loss: 0.0035979781, LR: 0.0010000000\n",
      "Epoch: 24/500, Loss: 0.0035134480, LR: 0.0010000000\n",
      "Epoch: 25/500, Loss: 0.0035303025, LR: 0.0010000000\n",
      "Epoch: 26/500, Loss: 0.0034993120, LR: 0.0010000000\n",
      "Epoch: 27/500, Loss: 0.0035749087, LR: 0.0010000000\n",
      "Epoch: 28/500, Loss: 0.0035007481, LR: 0.0010000000\n",
      "Epoch: 29/500, Loss: 0.0034801541, LR: 0.0010000000\n",
      "Epoch: 30/500, Loss: 0.0034653485, LR: 0.0010000000\n",
      "Epoch: 31/500, Loss: 0.0034629858, LR: 0.0010000000\n",
      "Epoch: 32/500, Loss: 0.0034572589, LR: 0.0010000000\n",
      "Epoch: 33/500, Loss: 0.0034290924, LR: 0.0010000000\n",
      "Epoch: 34/500, Loss: 0.0034256014, LR: 0.0010000000\n",
      "Epoch: 35/500, Loss: 0.0034276683, LR: 0.0010000000\n",
      "Epoch: 36/500, Loss: 0.0034118475, LR: 0.0010000000\n",
      "Epoch: 37/500, Loss: 0.0034106287, LR: 0.0010000000\n",
      "Epoch: 38/500, Loss: 0.0034001307, LR: 0.0010000000\n",
      "Epoch: 39/500, Loss: 0.0034014110, LR: 0.0010000000\n",
      "Epoch: 40/500, Loss: 0.0033846494, LR: 0.0010000000\n",
      "Epoch: 41/500, Loss: 0.0033838609, LR: 0.0010000000\n",
      "Epoch: 42/500, Loss: 0.0033764175, LR: 0.0010000000\n",
      "Epoch: 43/500, Loss: 0.0033814647, LR: 0.0010000000\n",
      "Epoch: 44/500, Loss: 0.0033739708, LR: 0.0010000000\n",
      "Epoch: 45/500, Loss: 0.0033805445, LR: 0.0010000000\n",
      "Epoch: 46/500, Loss: 0.0033636333, LR: 0.0010000000\n",
      "Epoch: 47/500, Loss: 0.0033646913, LR: 0.0010000000\n",
      "Epoch: 48/500, Loss: 0.0033713289, LR: 0.0010000000\n",
      "Epoch: 49/500, Loss: 0.0033619793, LR: 0.0010000000\n",
      "Epoch: 50/500, Loss: 0.0033554641, LR: 0.0010000000\n",
      "Epoch: 51/500, Loss: 0.0033573142, LR: 0.0010000000\n",
      "Epoch: 52/500, Loss: 0.0033570622, LR: 0.0010000000\n",
      "Epoch: 53/500, Loss: 0.0033501029, LR: 0.0010000000\n",
      "Epoch: 54/500, Loss: 0.0033511871, LR: 0.0010000000\n",
      "Epoch: 55/500, Loss: 0.0033462891, LR: 0.0010000000\n",
      "Epoch: 56/500, Loss: 0.0033492507, LR: 0.0010000000\n",
      "Epoch: 57/500, Loss: 0.0033505403, LR: 0.0010000000\n",
      "Epoch: 58/500, Loss: 0.0033503761, LR: 0.0010000000\n",
      "Epoch: 59/500, Loss: 0.0033449497, LR: 0.0010000000\n",
      "Epoch: 60/500, Loss: 0.0033364920, LR: 0.0010000000\n",
      "Epoch: 61/500, Loss: 0.0033465824, LR: 0.0010000000\n",
      "Epoch: 62/500, Loss: 0.0033460444, LR: 0.0010000000\n",
      "Epoch: 63/500, Loss: 0.0033430244, LR: 0.0010000000\n",
      "Epoch: 64/500, Loss: 0.0033414410, LR: 0.0010000000\n",
      "Epoch: 65/500, Loss: 0.0033376956, LR: 0.0010000000\n",
      "Epoch: 66/500, Loss: 0.0033417962, LR: 0.0005000000\n",
      "Epoch: 67/500, Loss: 0.0033100745, LR: 0.0005000000\n",
      "Epoch: 68/500, Loss: 0.0033061365, LR: 0.0005000000\n",
      "Epoch: 69/500, Loss: 0.0033058582, LR: 0.0005000000\n",
      "Epoch: 70/500, Loss: 0.0033107985, LR: 0.0005000000\n",
      "Epoch: 71/500, Loss: 0.0033087561, LR: 0.0005000000\n",
      "Epoch: 72/500, Loss: 0.0033110674, LR: 0.0005000000\n",
      "Epoch: 73/500, Loss: 0.0033052845, LR: 0.0005000000\n",
      "Epoch: 74/500, Loss: 0.0033064075, LR: 0.0005000000\n",
      "Epoch: 75/500, Loss: 0.0033037923, LR: 0.0005000000\n",
      "Epoch: 76/500, Loss: 0.0033056947, LR: 0.0005000000\n",
      "Epoch: 77/500, Loss: 0.0033026754, LR: 0.0005000000\n",
      "Epoch: 78/500, Loss: 0.0033059417, LR: 0.0005000000\n",
      "Epoch: 79/500, Loss: 0.0032998721, LR: 0.0005000000\n",
      "Epoch: 80/500, Loss: 0.0033017399, LR: 0.0005000000\n",
      "Epoch: 81/500, Loss: 0.0033026339, LR: 0.0005000000\n",
      "Epoch: 82/500, Loss: 0.0033006401, LR: 0.0005000000\n",
      "Epoch: 83/500, Loss: 0.0032996113, LR: 0.0005000000\n",
      "Epoch: 84/500, Loss: 0.0032976009, LR: 0.0005000000\n",
      "Epoch: 85/500, Loss: 0.0032972691, LR: 0.0005000000\n",
      "Epoch: 86/500, Loss: 0.0033012225, LR: 0.0005000000\n",
      "Epoch: 87/500, Loss: 0.0032985824, LR: 0.0005000000\n",
      "Epoch: 88/500, Loss: 0.0032991750, LR: 0.0005000000\n",
      "Epoch: 89/500, Loss: 0.0032987663, LR: 0.0005000000\n",
      "Epoch: 90/500, Loss: 0.0032952957, LR: 0.0005000000\n",
      "Epoch: 91/500, Loss: 0.0032967939, LR: 0.0005000000\n",
      "Epoch: 92/500, Loss: 0.0032960700, LR: 0.0005000000\n",
      "Epoch: 93/500, Loss: 0.0032969201, LR: 0.0005000000\n",
      "Epoch: 94/500, Loss: 0.0032956964, LR: 0.0005000000\n",
      "Epoch: 95/500, Loss: 0.0032938393, LR: 0.0005000000\n",
      "Epoch: 96/500, Loss: 0.0032901880, LR: 0.0005000000\n",
      "Epoch: 97/500, Loss: 0.0032931065, LR: 0.0005000000\n",
      "Epoch: 98/500, Loss: 0.0032973120, LR: 0.0005000000\n",
      "Epoch: 99/500, Loss: 0.0032971277, LR: 0.0005000000\n",
      "Epoch: 100/500, Loss: 0.0032921610, LR: 0.0005000000\n",
      "Epoch: 101/500, Loss: 0.0032905522, LR: 0.0005000000\n",
      "Epoch: 102/500, Loss: 0.0032921727, LR: 0.0002500000\n",
      "Epoch: 103/500, Loss: 0.0032766114, LR: 0.0002500000\n",
      "Epoch: 104/500, Loss: 0.0032755621, LR: 0.0002500000\n",
      "Epoch: 105/500, Loss: 0.0032764176, LR: 0.0002500000\n",
      "Epoch: 106/500, Loss: 0.0032696866, LR: 0.0002500000\n",
      "Epoch: 107/500, Loss: 0.0032740440, LR: 0.0002500000\n",
      "Epoch: 108/500, Loss: 0.0032752864, LR: 0.0002500000\n",
      "Epoch: 109/500, Loss: 0.0032745130, LR: 0.0002500000\n",
      "Epoch: 110/500, Loss: 0.0032698756, LR: 0.0002500000\n",
      "Epoch: 111/500, Loss: 0.0032726404, LR: 0.0002500000\n",
      "Epoch: 112/500, Loss: 0.0032681471, LR: 0.0002500000\n",
      "Epoch: 113/500, Loss: 0.0032696540, LR: 0.0002500000\n",
      "Epoch: 114/500, Loss: 0.0032698041, LR: 0.0002500000\n",
      "Epoch: 115/500, Loss: 0.0032672825, LR: 0.0002500000\n",
      "Epoch: 116/500, Loss: 0.0032699589, LR: 0.0002500000\n",
      "Epoch: 117/500, Loss: 0.0032675570, LR: 0.0002500000\n",
      "Epoch: 118/500, Loss: 0.0032671852, LR: 0.0002500000\n",
      "Epoch: 119/500, Loss: 0.0032677951, LR: 0.0002500000\n",
      "Epoch: 120/500, Loss: 0.0032656220, LR: 0.0002500000\n",
      "Epoch: 121/500, Loss: 0.0032672152, LR: 0.0002500000\n",
      "Epoch: 122/500, Loss: 0.0032661351, LR: 0.0002500000\n",
      "Epoch: 123/500, Loss: 0.0032634863, LR: 0.0002500000\n",
      "Epoch: 124/500, Loss: 0.0032653403, LR: 0.0002500000\n",
      "Epoch: 125/500, Loss: 0.0032656499, LR: 0.0002500000\n",
      "Epoch: 126/500, Loss: 0.0032639531, LR: 0.0002500000\n",
      "Epoch: 127/500, Loss: 0.0032590082, LR: 0.0002500000\n",
      "Epoch: 128/500, Loss: 0.0032643662, LR: 0.0002500000\n",
      "Epoch: 129/500, Loss: 0.0032650021, LR: 0.0002500000\n",
      "Epoch: 130/500, Loss: 0.0032591209, LR: 0.0002500000\n",
      "Epoch: 131/500, Loss: 0.0032644236, LR: 0.0002500000\n",
      "Epoch: 132/500, Loss: 0.0032649168, LR: 0.0002500000\n",
      "Epoch: 133/500, Loss: 0.0032602569, LR: 0.0001250000\n",
      "Epoch: 134/500, Loss: 0.0032494925, LR: 0.0001250000\n",
      "Epoch: 135/500, Loss: 0.0032439755, LR: 0.0001250000\n",
      "Epoch: 136/500, Loss: 0.0032467381, LR: 0.0001250000\n",
      "Epoch: 137/500, Loss: 0.0032450544, LR: 0.0001250000\n",
      "Epoch: 138/500, Loss: 0.0032481497, LR: 0.0001250000\n",
      "Epoch: 139/500, Loss: 0.0032480523, LR: 0.0001250000\n",
      "Epoch: 140/500, Loss: 0.0032445291, LR: 0.0001250000\n",
      "Epoch: 141/500, Loss: 0.0032432648, LR: 0.0001250000\n",
      "Epoch: 142/500, Loss: 0.0032445209, LR: 0.0001250000\n",
      "Epoch: 143/500, Loss: 0.0032429675, LR: 0.0001250000\n",
      "Epoch: 144/500, Loss: 0.0032436153, LR: 0.0001250000\n",
      "Epoch: 145/500, Loss: 0.0032424073, LR: 0.0001250000\n",
      "Epoch: 146/500, Loss: 0.0032433144, LR: 0.0001250000\n",
      "Epoch: 147/500, Loss: 0.0032417982, LR: 0.0001250000\n",
      "Epoch: 148/500, Loss: 0.0032422195, LR: 0.0001250000\n",
      "Epoch: 149/500, Loss: 0.0032410761, LR: 0.0001250000\n",
      "Epoch: 150/500, Loss: 0.0032388163, LR: 0.0001250000\n",
      "Epoch: 151/500, Loss: 0.0032391565, LR: 0.0001250000\n",
      "Epoch: 152/500, Loss: 0.0032440100, LR: 0.0001250000\n",
      "Epoch: 153/500, Loss: 0.0032403476, LR: 0.0001250000\n",
      "Epoch: 154/500, Loss: 0.0032389318, LR: 0.0001250000\n",
      "Epoch: 155/500, Loss: 0.0032406566, LR: 0.0001250000\n",
      "Epoch: 156/500, Loss: 0.0032356999, LR: 0.0001250000\n",
      "Epoch: 157/500, Loss: 0.0032364737, LR: 0.0001250000\n",
      "Epoch: 158/500, Loss: 0.0032394038, LR: 0.0001250000\n",
      "Epoch: 159/500, Loss: 0.0032364304, LR: 0.0001250000\n",
      "Epoch: 160/500, Loss: 0.0032359266, LR: 0.0001250000\n",
      "Epoch: 161/500, Loss: 0.0032352892, LR: 0.0001250000\n",
      "Epoch: 162/500, Loss: 0.0032367520, LR: 0.0001250000\n",
      "Epoch: 163/500, Loss: 0.0032357169, LR: 0.0001250000\n",
      "Epoch: 164/500, Loss: 0.0032341048, LR: 0.0001250000\n",
      "Epoch: 165/500, Loss: 0.0032383152, LR: 0.0001250000\n",
      "Epoch: 166/500, Loss: 0.0032367268, LR: 0.0001250000\n",
      "Epoch: 167/500, Loss: 0.0032343229, LR: 0.0001250000\n",
      "Epoch: 168/500, Loss: 0.0032313771, LR: 0.0001250000\n",
      "Epoch: 169/500, Loss: 0.0032303707, LR: 0.0001250000\n",
      "Epoch: 170/500, Loss: 0.0032285308, LR: 0.0001250000\n",
      "Epoch: 171/500, Loss: 0.0032295476, LR: 0.0001250000\n",
      "Epoch: 172/500, Loss: 0.0032338360, LR: 0.0001250000\n",
      "Epoch: 173/500, Loss: 0.0032300115, LR: 0.0001250000\n",
      "Epoch: 174/500, Loss: 0.0032317192, LR: 0.0001250000\n",
      "Epoch: 175/500, Loss: 0.0032312136, LR: 0.0001250000\n",
      "Epoch: 176/500, Loss: 0.0032297398, LR: 0.0000625000\n",
      "Epoch: 177/500, Loss: 0.0032201477, LR: 0.0000625000\n",
      "Epoch: 178/500, Loss: 0.0032210779, LR: 0.0000625000\n",
      "Epoch: 179/500, Loss: 0.0032216252, LR: 0.0000625000\n",
      "Epoch: 180/500, Loss: 0.0032164368, LR: 0.0000625000\n",
      "Epoch: 181/500, Loss: 0.0032181088, LR: 0.0000625000\n",
      "Epoch: 182/500, Loss: 0.0032195213, LR: 0.0000625000\n",
      "Epoch: 183/500, Loss: 0.0032169318, LR: 0.0000625000\n",
      "Epoch: 184/500, Loss: 0.0032152999, LR: 0.0000625000\n",
      "Epoch: 185/500, Loss: 0.0032187371, LR: 0.0000625000\n",
      "Epoch: 186/500, Loss: 0.0032143927, LR: 0.0000625000\n",
      "Epoch: 187/500, Loss: 0.0032162155, LR: 0.0000625000\n",
      "Epoch: 188/500, Loss: 0.0032153698, LR: 0.0000625000\n",
      "Epoch: 189/500, Loss: 0.0032166407, LR: 0.0000625000\n",
      "Epoch: 190/500, Loss: 0.0032168876, LR: 0.0000625000\n",
      "Epoch: 191/500, Loss: 0.0032165214, LR: 0.0000625000\n",
      "Epoch: 192/500, Loss: 0.0032162067, LR: 0.0000312500\n",
      "Epoch: 193/500, Loss: 0.0032111920, LR: 0.0000312500\n",
      "Epoch: 194/500, Loss: 0.0032101448, LR: 0.0000312500\n",
      "Epoch: 195/500, Loss: 0.0032087311, LR: 0.0000312500\n",
      "Epoch: 196/500, Loss: 0.0032084280, LR: 0.0000312500\n",
      "Epoch: 197/500, Loss: 0.0032061761, LR: 0.0000312500\n",
      "Epoch: 198/500, Loss: 0.0032099932, LR: 0.0000312500\n",
      "Epoch: 199/500, Loss: 0.0032113002, LR: 0.0000312500\n",
      "Epoch: 200/500, Loss: 0.0032097304, LR: 0.0000312500\n",
      "Epoch: 201/500, Loss: 0.0032070924, LR: 0.0000312500\n",
      "Epoch: 202/500, Loss: 0.0032051787, LR: 0.0000312500\n",
      "Epoch: 203/500, Loss: 0.0032070626, LR: 0.0000312500\n",
      "Epoch: 204/500, Loss: 0.0032092349, LR: 0.0000312500\n",
      "Epoch: 205/500, Loss: 0.0032100945, LR: 0.0000312500\n",
      "Epoch: 206/500, Loss: 0.0032057028, LR: 0.0000312500\n",
      "Epoch: 207/500, Loss: 0.0032066882, LR: 0.0000312500\n",
      "Epoch: 208/500, Loss: 0.0032105093, LR: 0.0000156250\n",
      "Epoch: 209/500, Loss: 0.0032042841, LR: 0.0000156250\n",
      "Epoch: 210/500, Loss: 0.0032000480, LR: 0.0000156250\n",
      "Epoch: 211/500, Loss: 0.0032043319, LR: 0.0000156250\n",
      "Epoch: 212/500, Loss: 0.0032020097, LR: 0.0000156250\n",
      "Epoch: 213/500, Loss: 0.0032052917, LR: 0.0000156250\n",
      "Epoch: 214/500, Loss: 0.0032047009, LR: 0.0000156250\n",
      "Epoch: 215/500, Loss: 0.0032059061, LR: 0.0000156250\n",
      "Epoch: 216/500, Loss: 0.0032017563, LR: 0.0000078125\n",
      "Epoch: 217/500, Loss: 0.0031993637, LR: 0.0000078125\n",
      "Epoch: 218/500, Loss: 0.0032011038, LR: 0.0000078125\n",
      "Epoch: 219/500, Loss: 0.0032007582, LR: 0.0000078125\n",
      "Epoch: 220/500, Loss: 0.0032014767, LR: 0.0000078125\n",
      "Epoch: 221/500, Loss: 0.0032037095, LR: 0.0000078125\n",
      "Epoch: 222/500, Loss: 0.0031965873, LR: 0.0000078125\n",
      "Epoch: 223/500, Loss: 0.0031984742, LR: 0.0000078125\n",
      "Epoch: 224/500, Loss: 0.0031986802, LR: 0.0000078125\n",
      "Epoch: 225/500, Loss: 0.0031982461, LR: 0.0000078125\n",
      "Epoch: 226/500, Loss: 0.0032033760, LR: 0.0000078125\n",
      "Epoch: 227/500, Loss: 0.0031998142, LR: 0.0000078125\n",
      "Epoch: 228/500, Loss: 0.0032003377, LR: 0.0000039063\n",
      "Epoch: 229/500, Loss: 0.0032011858, LR: 0.0000039063\n",
      "Epoch: 230/500, Loss: 0.0032017425, LR: 0.0000039063\n",
      "Epoch: 231/500, Loss: 0.0031990606, LR: 0.0000039063\n",
      "Epoch: 232/500, Loss: 0.0032013002, LR: 0.0000039063\n",
      "Epoch: 233/500, Loss: 0.0032011991, LR: 0.0000039063\n",
      "Epoch: 234/500, Loss: 0.0032013578, LR: 0.0000019531\n",
      "Epoch: 235/500, Loss: 0.0032023982, LR: 0.0000019531\n",
      "Epoch: 236/500, Loss: 0.0032003299, LR: 0.0000019531\n",
      "Epoch: 237/500, Loss: 0.0031981480, LR: 0.0000019531\n",
      "Epoch: 238/500, Loss: 0.0031981613, LR: 0.0000019531\n",
      "Epoch: 239/500, Loss: 0.0031975370, LR: 0.0000019531\n",
      "Epoch: 240/500, Loss: 0.0032020448, LR: 0.0000009766\n",
      "Epoch: 241/500, Loss: 0.0031975786, LR: 0.0000009766\n",
      "Epoch: 242/500, Loss: 0.0032036315, LR: 0.0000009766\n",
      "Epoch: 243/500, Loss: 0.0031989642, LR: 0.0000009766\n",
      "Epoch: 244/500, Loss: 0.0031998839, LR: 0.0000009766\n",
      "Epoch: 245/500, Loss: 0.0032002139, LR: 0.0000009766\n",
      "Epoch: 246/500, Loss: 0.0031981643, LR: 0.0000004883\n",
      "Epoch: 247/500, Loss: 0.0031963495, LR: 0.0000004883\n",
      "Epoch: 248/500, Loss: 0.0032028782, LR: 0.0000004883\n",
      "Epoch: 249/500, Loss: 0.0031971006, LR: 0.0000004883\n",
      "Epoch: 250/500, Loss: 0.0032014694, LR: 0.0000004883\n",
      "Epoch: 251/500, Loss: 0.0031980672, LR: 0.0000004883\n",
      "Epoch: 252/500, Loss: 0.0032030280, LR: 0.0000002441\n",
      "Epoch: 253/500, Loss: 0.0032026024, LR: 0.0000002441\n",
      "Epoch: 254/500, Loss: 0.0031996655, LR: 0.0000002441\n",
      "Epoch: 255/500, Loss: 0.0031972618, LR: 0.0000002441\n",
      "Epoch: 256/500, Loss: 0.0032009333, LR: 0.0000002441\n",
      "Epoch: 257/500, Loss: 0.0032004307, LR: 0.0000002441\n",
      "Epoch: 258/500, Loss: 0.0031981351, LR: 0.0000001221\n",
      "Epoch: 259/500, Loss: 0.0032015745, LR: 0.0000001221\n",
      "Epoch: 260/500, Loss: 0.0031991832, LR: 0.0000001221\n",
      "Epoch: 261/500, Loss: 0.0031983744, LR: 0.0000001221\n",
      "Epoch: 262/500, Loss: 0.0031988265, LR: 0.0000001221\n",
      "Epoch: 263/500, Loss: 0.0032019716, LR: 0.0000001221\n",
      "Epoch: 264/500, Loss: 0.0031977370, LR: 0.0000000610\n",
      "Epoch: 265/500, Loss: 0.0032019430, LR: 0.0000000610\n",
      "Epoch: 266/500, Loss: 0.0031961513, LR: 0.0000000610\n",
      "Epoch: 267/500, Loss: 0.0032005809, LR: 0.0000000610\n",
      "Epoch: 268/500, Loss: 0.0031966512, LR: 0.0000000610\n",
      "Epoch: 269/500, Loss: 0.0031953870, LR: 0.0000000610\n",
      "Epoch: 270/500, Loss: 0.0031981873, LR: 0.0000000610\n",
      "Epoch: 271/500, Loss: 0.0031966510, LR: 0.0000000610\n",
      "Epoch: 272/500, Loss: 0.0032004868, LR: 0.0000000610\n",
      "Epoch: 273/500, Loss: 0.0031981573, LR: 0.0000000610\n",
      "Epoch: 274/500, Loss: 0.0031990054, LR: 0.0000000610\n",
      "Epoch: 275/500, Loss: 0.0032004356, LR: 0.0000000305\n",
      "Epoch: 276/500, Loss: 0.0031980695, LR: 0.0000000305\n",
      "Epoch: 277/500, Loss: 0.0032001693, LR: 0.0000000305\n",
      "Epoch: 278/500, Loss: 0.0032026490, LR: 0.0000000305\n",
      "Epoch: 279/500, Loss: 0.0031977129, LR: 0.0000000305\n",
      "Epoch: 280/500, Loss: 0.0031968578, LR: 0.0000000305\n",
      "Epoch: 281/500, Loss: 0.0032035233, LR: 0.0000000153\n",
      "Epoch: 282/500, Loss: 0.0032004486, LR: 0.0000000153\n",
      "Epoch: 283/500, Loss: 0.0031947233, LR: 0.0000000153\n",
      "Epoch: 284/500, Loss: 0.0031969713, LR: 0.0000000153\n",
      "Epoch: 285/500, Loss: 0.0031982516, LR: 0.0000000153\n",
      "Epoch: 286/500, Loss: 0.0031981351, LR: 0.0000000153\n",
      "Epoch: 287/500, Loss: 0.0031973470, LR: 0.0000000153\n",
      "Epoch: 288/500, Loss: 0.0031970049, LR: 0.0000000153\n",
      "Epoch: 289/500, Loss: 0.0031968442, LR: 0.0000000153\n",
      "Epoch: 290/500, Loss: 0.0032000680, LR: 0.0000000153\n",
      "Epoch: 291/500, Loss: 0.0031985946, LR: 0.0000000153\n",
      "Epoch: 292/500, Loss: 0.0031953651, LR: 0.0000000153\n",
      "Epoch: 293/500, Loss: 0.0032000194, LR: 0.0000000153\n",
      "Epoch: 294/500, Loss: 0.0031972003, LR: 0.0000000153\n",
      "Epoch: 295/500, Loss: 0.0032005472, LR: 0.0000000153\n",
      "Epoch: 296/500, Loss: 0.0031972737, LR: 0.0000000153\n",
      "Epoch: 297/500, Loss: 0.0031981847, LR: 0.0000000153\n",
      "Epoch: 298/500, Loss: 0.0031979775, LR: 0.0000000153\n",
      "Epoch: 299/500, Loss: 0.0031979875, LR: 0.0000000153\n",
      "Epoch: 300/500, Loss: 0.0031997752, LR: 0.0000000153\n",
      "Epoch: 301/500, Loss: 0.0031980728, LR: 0.0000000153\n",
      "Epoch: 302/500, Loss: 0.0031976575, LR: 0.0000000153\n",
      "Epoch: 303/500, Loss: 0.0031969079, LR: 0.0000000153\n",
      "Epoch: 304/500, Loss: 0.0031989196, LR: 0.0000000153\n",
      "Epoch: 305/500, Loss: 0.0032006171, LR: 0.0000000153\n",
      "Epoch: 306/500, Loss: 0.0031963345, LR: 0.0000000153\n",
      "Epoch: 307/500, Loss: 0.0031976694, LR: 0.0000000153\n",
      "Epoch: 308/500, Loss: 0.0032014101, LR: 0.0000000153\n",
      "Epoch: 309/500, Loss: 0.0031951744, LR: 0.0000000153\n",
      "Epoch: 310/500, Loss: 0.0031966476, LR: 0.0000000153\n",
      "Epoch: 311/500, Loss: 0.0031949749, LR: 0.0000000153\n",
      "Epoch: 312/500, Loss: 0.0032009982, LR: 0.0000000153\n",
      "Epoch: 313/500, Loss: 0.0031979196, LR: 0.0000000153\n",
      "Epoch: 314/500, Loss: 0.0031984172, LR: 0.0000000153\n",
      "Epoch: 315/500, Loss: 0.0032002215, LR: 0.0000000153\n",
      "Epoch: 316/500, Loss: 0.0031963496, LR: 0.0000000153\n",
      "Epoch: 317/500, Loss: 0.0032018063, LR: 0.0000000153\n",
      "Epoch: 318/500, Loss: 0.0031976312, LR: 0.0000000153\n",
      "Epoch: 319/500, Loss: 0.0031937140, LR: 0.0000000153\n",
      "Epoch: 320/500, Loss: 0.0031966528, LR: 0.0000000153\n",
      "Epoch: 321/500, Loss: 0.0031977909, LR: 0.0000000153\n",
      "Epoch: 322/500, Loss: 0.0031996791, LR: 0.0000000153\n",
      "Epoch: 323/500, Loss: 0.0031992552, LR: 0.0000000153\n",
      "Epoch: 324/500, Loss: 0.0031983631, LR: 0.0000000153\n",
      "Epoch: 325/500, Loss: 0.0032009017, LR: 0.0000000153\n",
      "Epoch: 326/500, Loss: 0.0031984041, LR: 0.0000000153\n",
      "Epoch: 327/500, Loss: 0.0031971784, LR: 0.0000000153\n",
      "Epoch: 328/500, Loss: 0.0031974427, LR: 0.0000000153\n",
      "Epoch: 329/500, Loss: 0.0032015092, LR: 0.0000000153\n",
      "Epoch: 330/500, Loss: 0.0031999832, LR: 0.0000000153\n",
      "Epoch: 331/500, Loss: 0.0031977714, LR: 0.0000000153\n",
      "Epoch: 332/500, Loss: 0.0031981066, LR: 0.0000000153\n",
      "Epoch: 333/500, Loss: 0.0032011936, LR: 0.0000000153\n",
      "Epoch: 334/500, Loss: 0.0031955543, LR: 0.0000000153\n",
      "Epoch: 335/500, Loss: 0.0031988146, LR: 0.0000000153\n",
      "Epoch: 336/500, Loss: 0.0031935787, LR: 0.0000000153\n",
      "Epoch: 337/500, Loss: 0.0032005046, LR: 0.0000000153\n",
      "Epoch: 338/500, Loss: 0.0031992854, LR: 0.0000000153\n",
      "Epoch: 339/500, Loss: 0.0032011205, LR: 0.0000000153\n",
      "Epoch: 340/500, Loss: 0.0032008111, LR: 0.0000000153\n",
      "Epoch: 341/500, Loss: 0.0032012030, LR: 0.0000000153\n",
      "Epoch: 342/500, Loss: 0.0031986706, LR: 0.0000000153\n",
      "Epoch: 343/500, Loss: 0.0031988266, LR: 0.0000000153\n",
      "Epoch: 344/500, Loss: 0.0031969008, LR: 0.0000000153\n",
      "Epoch: 345/500, Loss: 0.0031988748, LR: 0.0000000153\n",
      "Epoch: 346/500, Loss: 0.0031965369, LR: 0.0000000153\n",
      "Epoch: 347/500, Loss: 0.0031976419, LR: 0.0000000153\n",
      "Epoch: 348/500, Loss: 0.0031964009, LR: 0.0000000153\n",
      "Epoch: 349/500, Loss: 0.0031969374, LR: 0.0000000153\n",
      "Epoch: 350/500, Loss: 0.0031999780, LR: 0.0000000153\n",
      "Epoch: 351/500, Loss: 0.0031955689, LR: 0.0000000153\n",
      "Epoch: 352/500, Loss: 0.0031975567, LR: 0.0000000153\n",
      "Epoch: 353/500, Loss: 0.0031972229, LR: 0.0000000153\n",
      "Epoch: 354/500, Loss: 0.0031995031, LR: 0.0000000153\n",
      "Epoch: 355/500, Loss: 0.0031988136, LR: 0.0000000153\n",
      "Epoch: 356/500, Loss: 0.0031990797, LR: 0.0000000153\n",
      "Epoch: 357/500, Loss: 0.0031960036, LR: 0.0000000153\n",
      "Epoch: 358/500, Loss: 0.0031972900, LR: 0.0000000153\n",
      "Epoch: 359/500, Loss: 0.0032001944, LR: 0.0000000153\n",
      "Epoch: 360/500, Loss: 0.0031987067, LR: 0.0000000153\n",
      "Epoch: 361/500, Loss: 0.0031992843, LR: 0.0000000153\n",
      "Epoch: 362/500, Loss: 0.0031948698, LR: 0.0000000153\n",
      "Epoch: 363/500, Loss: 0.0031988226, LR: 0.0000000153\n",
      "Epoch: 364/500, Loss: 0.0031985988, LR: 0.0000000153\n",
      "Epoch: 365/500, Loss: 0.0032012591, LR: 0.0000000153\n",
      "Epoch: 366/500, Loss: 0.0031995151, LR: 0.0000000153\n",
      "Epoch: 367/500, Loss: 0.0031994005, LR: 0.0000000153\n",
      "Epoch: 368/500, Loss: 0.0031999746, LR: 0.0000000153\n",
      "Epoch: 369/500, Loss: 0.0031985959, LR: 0.0000000153\n",
      "Epoch: 370/500, Loss: 0.0031976466, LR: 0.0000000153\n",
      "Epoch: 371/500, Loss: 0.0031959979, LR: 0.0000000153\n",
      "Epoch: 372/500, Loss: 0.0032002426, LR: 0.0000000153\n",
      "Epoch: 373/500, Loss: 0.0032007235, LR: 0.0000000153\n",
      "Epoch: 374/500, Loss: 0.0031991978, LR: 0.0000000153\n",
      "Epoch: 375/500, Loss: 0.0031928607, LR: 0.0000000153\n",
      "Epoch: 376/500, Loss: 0.0031960803, LR: 0.0000000153\n",
      "Epoch: 377/500, Loss: 0.0031974017, LR: 0.0000000153\n",
      "Epoch: 378/500, Loss: 0.0031997308, LR: 0.0000000153\n",
      "Epoch: 379/500, Loss: 0.0031969926, LR: 0.0000000153\n",
      "Epoch: 380/500, Loss: 0.0031991361, LR: 0.0000000153\n",
      "Epoch: 381/500, Loss: 0.0031956143, LR: 0.0000000153\n",
      "Epoch: 382/500, Loss: 0.0031977550, LR: 0.0000000153\n",
      "Epoch: 383/500, Loss: 0.0031991259, LR: 0.0000000153\n",
      "Epoch: 384/500, Loss: 0.0031997100, LR: 0.0000000153\n",
      "Epoch: 385/500, Loss: 0.0032005812, LR: 0.0000000153\n",
      "Epoch: 386/500, Loss: 0.0032029117, LR: 0.0000000153\n",
      "Epoch: 387/500, Loss: 0.0031994934, LR: 0.0000000153\n",
      "Epoch: 388/500, Loss: 0.0031983876, LR: 0.0000000153\n",
      "Epoch: 389/500, Loss: 0.0031999050, LR: 0.0000000153\n",
      "Epoch: 390/500, Loss: 0.0031961566, LR: 0.0000000153\n",
      "Epoch: 391/500, Loss: 0.0031962977, LR: 0.0000000153\n",
      "Epoch: 392/500, Loss: 0.0032002977, LR: 0.0000000153\n",
      "Epoch: 393/500, Loss: 0.0031952083, LR: 0.0000000153\n",
      "Epoch: 394/500, Loss: 0.0032028252, LR: 0.0000000153\n",
      "Epoch: 395/500, Loss: 0.0031961724, LR: 0.0000000153\n",
      "Epoch: 396/500, Loss: 0.0032001306, LR: 0.0000000153\n",
      "Epoch: 397/500, Loss: 0.0031981737, LR: 0.0000000153\n",
      "Epoch: 398/500, Loss: 0.0031993276, LR: 0.0000000153\n",
      "Epoch: 399/500, Loss: 0.0031964487, LR: 0.0000000153\n",
      "Epoch: 400/500, Loss: 0.0031946239, LR: 0.0000000153\n",
      "Epoch: 401/500, Loss: 0.0031988021, LR: 0.0000000153\n",
      "Epoch: 402/500, Loss: 0.0031964891, LR: 0.0000000153\n",
      "Epoch: 403/500, Loss: 0.0031977608, LR: 0.0000000153\n",
      "Epoch: 404/500, Loss: 0.0031966576, LR: 0.0000000153\n",
      "Epoch: 405/500, Loss: 0.0032038342, LR: 0.0000000153\n",
      "Epoch: 406/500, Loss: 0.0031990509, LR: 0.0000000153\n",
      "Epoch: 407/500, Loss: 0.0031995272, LR: 0.0000000153\n",
      "Epoch: 408/500, Loss: 0.0031984449, LR: 0.0000000153\n",
      "Epoch: 409/500, Loss: 0.0031997689, LR: 0.0000000153\n",
      "Epoch: 410/500, Loss: 0.0032000023, LR: 0.0000000153\n",
      "Epoch: 411/500, Loss: 0.0031972586, LR: 0.0000000153\n",
      "Epoch: 412/500, Loss: 0.0031976131, LR: 0.0000000153\n",
      "Epoch: 413/500, Loss: 0.0031989322, LR: 0.0000000153\n",
      "Epoch: 414/500, Loss: 0.0031984415, LR: 0.0000000153\n",
      "Epoch: 415/500, Loss: 0.0031999488, LR: 0.0000000153\n",
      "Epoch: 416/500, Loss: 0.0031970740, LR: 0.0000000153\n",
      "Epoch: 417/500, Loss: 0.0032028861, LR: 0.0000000153\n",
      "Epoch: 418/500, Loss: 0.0031963153, LR: 0.0000000153\n",
      "Epoch: 419/500, Loss: 0.0031982443, LR: 0.0000000153\n",
      "Epoch: 420/500, Loss: 0.0031957216, LR: 0.0000000153\n",
      "Epoch: 421/500, Loss: 0.0031936760, LR: 0.0000000153\n",
      "Epoch: 422/500, Loss: 0.0032009328, LR: 0.0000000153\n",
      "Epoch: 423/500, Loss: 0.0031963392, LR: 0.0000000153\n",
      "Epoch: 424/500, Loss: 0.0031980680, LR: 0.0000000153\n",
      "Epoch: 425/500, Loss: 0.0032016871, LR: 0.0000000153\n",
      "Epoch: 426/500, Loss: 0.0031991753, LR: 0.0000000153\n",
      "Epoch: 427/500, Loss: 0.0032010945, LR: 0.0000000153\n",
      "Epoch: 428/500, Loss: 0.0032006749, LR: 0.0000000153\n",
      "Epoch: 429/500, Loss: 0.0032023753, LR: 0.0000000153\n",
      "Epoch: 430/500, Loss: 0.0031982924, LR: 0.0000000153\n",
      "Epoch: 431/500, Loss: 0.0031988983, LR: 0.0000000153\n",
      "Epoch: 432/500, Loss: 0.0032008609, LR: 0.0000000153\n",
      "Epoch: 433/500, Loss: 0.0031967807, LR: 0.0000000153\n",
      "Epoch: 434/500, Loss: 0.0031979081, LR: 0.0000000153\n",
      "Epoch: 435/500, Loss: 0.0031984354, LR: 0.0000000153\n",
      "Epoch: 436/500, Loss: 0.0031971401, LR: 0.0000000153\n",
      "Epoch: 437/500, Loss: 0.0031974722, LR: 0.0000000153\n",
      "Epoch: 438/500, Loss: 0.0031951504, LR: 0.0000000153\n",
      "Epoch: 439/500, Loss: 0.0032046980, LR: 0.0000000153\n",
      "Epoch: 440/500, Loss: 0.0031966074, LR: 0.0000000153\n",
      "Epoch: 441/500, Loss: 0.0031958456, LR: 0.0000000153\n",
      "Epoch: 442/500, Loss: 0.0031987566, LR: 0.0000000153\n",
      "Epoch: 443/500, Loss: 0.0031984138, LR: 0.0000000153\n",
      "Epoch: 444/500, Loss: 0.0032013477, LR: 0.0000000153\n",
      "Epoch: 445/500, Loss: 0.0031966733, LR: 0.0000000153\n",
      "Epoch: 446/500, Loss: 0.0032038993, LR: 0.0000000153\n",
      "Epoch: 447/500, Loss: 0.0032001757, LR: 0.0000000153\n",
      "Epoch: 448/500, Loss: 0.0031983142, LR: 0.0000000153\n",
      "Epoch: 449/500, Loss: 0.0032001001, LR: 0.0000000153\n",
      "Epoch: 450/500, Loss: 0.0032024182, LR: 0.0000000153\n",
      "Epoch: 451/500, Loss: 0.0031996703, LR: 0.0000000153\n",
      "Epoch: 452/500, Loss: 0.0032000313, LR: 0.0000000153\n",
      "Epoch: 453/500, Loss: 0.0031981269, LR: 0.0000000153\n",
      "Epoch: 454/500, Loss: 0.0031988047, LR: 0.0000000153\n",
      "Epoch: 455/500, Loss: 0.0032046654, LR: 0.0000000153\n",
      "Epoch: 456/500, Loss: 0.0032008382, LR: 0.0000000153\n",
      "Epoch: 457/500, Loss: 0.0031987947, LR: 0.0000000153\n",
      "Epoch: 458/500, Loss: 0.0032022268, LR: 0.0000000153\n",
      "Epoch: 459/500, Loss: 0.0031963094, LR: 0.0000000153\n",
      "Epoch: 460/500, Loss: 0.0031994427, LR: 0.0000000153\n",
      "Epoch: 461/500, Loss: 0.0032012365, LR: 0.0000000153\n",
      "Epoch: 462/500, Loss: 0.0031972894, LR: 0.0000000153\n",
      "Epoch: 463/500, Loss: 0.0031959089, LR: 0.0000000153\n",
      "Epoch: 464/500, Loss: 0.0031982071, LR: 0.0000000153\n",
      "Epoch: 465/500, Loss: 0.0031969918, LR: 0.0000000153\n",
      "Epoch: 466/500, Loss: 0.0031995030, LR: 0.0000000153\n",
      "Epoch: 467/500, Loss: 0.0032009554, LR: 0.0000000153\n",
      "Epoch: 468/500, Loss: 0.0032004793, LR: 0.0000000153\n",
      "Epoch: 469/500, Loss: 0.0031972209, LR: 0.0000000153\n",
      "Epoch: 470/500, Loss: 0.0031994068, LR: 0.0000000153\n",
      "Epoch: 471/500, Loss: 0.0031990226, LR: 0.0000000153\n",
      "Epoch: 472/500, Loss: 0.0031959436, LR: 0.0000000153\n",
      "Epoch: 473/500, Loss: 0.0031956631, LR: 0.0000000153\n",
      "Epoch: 474/500, Loss: 0.0031982913, LR: 0.0000000153\n",
      "Epoch: 475/500, Loss: 0.0031996030, LR: 0.0000000153\n",
      "Epoch: 476/500, Loss: 0.0032017953, LR: 0.0000000153\n",
      "Epoch: 477/500, Loss: 0.0031985556, LR: 0.0000000153\n",
      "Epoch: 478/500, Loss: 0.0031957336, LR: 0.0000000153\n",
      "Epoch: 479/500, Loss: 0.0031986378, LR: 0.0000000153\n",
      "Epoch: 480/500, Loss: 0.0031996684, LR: 0.0000000153\n",
      "Epoch: 481/500, Loss: 0.0032012537, LR: 0.0000000153\n",
      "Epoch: 482/500, Loss: 0.0032030803, LR: 0.0000000153\n",
      "Epoch: 483/500, Loss: 0.0031967732, LR: 0.0000000153\n",
      "Epoch: 484/500, Loss: 0.0032021255, LR: 0.0000000153\n",
      "Epoch: 485/500, Loss: 0.0031979270, LR: 0.0000000153\n",
      "Epoch: 486/500, Loss: 0.0031991304, LR: 0.0000000153\n",
      "Epoch: 487/500, Loss: 0.0032002100, LR: 0.0000000153\n",
      "Epoch: 488/500, Loss: 0.0031990583, LR: 0.0000000153\n",
      "Epoch: 489/500, Loss: 0.0031985748, LR: 0.0000000153\n",
      "Epoch: 490/500, Loss: 0.0031993051, LR: 0.0000000153\n",
      "Epoch: 491/500, Loss: 0.0032006486, LR: 0.0000000153\n",
      "Epoch: 492/500, Loss: 0.0031986693, LR: 0.0000000153\n",
      "Epoch: 493/500, Loss: 0.0032004983, LR: 0.0000000153\n",
      "Epoch: 494/500, Loss: 0.0031965708, LR: 0.0000000153\n",
      "Epoch: 495/500, Loss: 0.0031977232, LR: 0.0000000153\n",
      "Epoch: 496/500, Loss: 0.0031985470, LR: 0.0000000153\n",
      "Epoch: 497/500, Loss: 0.0031975022, LR: 0.0000000153\n",
      "Epoch: 498/500, Loss: 0.0031983689, LR: 0.0000000153\n",
      "Epoch: 499/500, Loss: 0.0031991431, LR: 0.0000000153\n",
      "Epoch: 500/500, Loss: 0.0031982750, LR: 0.0000000153\n"
     ]
    }
   ],
   "source": [
    "model = CursorRNN(input_size, hidden_size, output_size, num_layers=lstm_layers).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()  # loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # optimizer\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    ")  # learning rate scheduler for better convergence\n",
    "\n",
    "# for best model tracking\n",
    "best_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(\n",
    "            device\n",
    "        )  # moving batch to gpu or cpu based on availability\n",
    "        optimizer.zero_grad()  # zeroing the gradients\n",
    "\n",
    "        y_pred, _ = model(x_batch)  # forward pass\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)  # calculating loss\n",
    "        loss.backward()  # backpropagation\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), max_norm=1.0\n",
    "        )  # gradient clipping to prevent exploding gradients\n",
    "\n",
    "        optimizer.step()  # updating weights\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    scheduler.step(avg_loss)  # updating learning rate based on loss\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "\n",
    "    print(\n",
    "        f'Epoch: {epoch+1}/{epochs}, Loss: {avg_loss:.10f}, LR: {optimizer.param_groups[0][\"lr\"]:.10f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"cursor-rnn-model-{time.strftime('%Y-%m-%d-%H:%M:%S')}.pth\"\n",
    "model_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"models\", model_name)\n",
    "torch.save(best_model_state, model_path)\n",
    "print(f\"Model saved at {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bumblebee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
