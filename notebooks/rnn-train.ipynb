{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Training Notebook  \n",
    "\n",
    "This notebook trains an AI model to predict mouse cursor movement paths. The model is built using a Recurrent Neural Network (RNN) with an LSTM layer for attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/arpanbhandari/Documents/coding/bumblebee/data/processed/cleaned-data-39-steps-merged-prepared-data-2025-03-08-16:27:23.json exists: Yes\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(\n",
    "    os.path.dirname(os.getcwd()),\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"cleaned-data-39-steps-merged-prepared-data-2025-03-08-16:27:23.json\",\n",
    ")  # this path is for the cleaned data; must be changed accordingly\n",
    "\n",
    "print(\n",
    "    f\"{dataset_path} exists: {\"Yes\" if os.path.exists(dataset_path) else 'No'}\"\n",
    ")  ## must be Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = open(dataset_path, \"r\")\n",
    "dataset_json = json.load(dataset_file)\n",
    "dataset_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_coordinate = 0\n",
    "max_coordinate = 4096  # This will allow the model to navigate in 4k resolution\n",
    "\n",
    "\n",
    "def normalize(data):  # Normalizing the data to be in the range [0, 1]\n",
    "    return (data - min_coordinate) / (max_coordinate - min_coordinate)\n",
    "\n",
    "\n",
    "def denormalize(\n",
    "    data,\n",
    "):  # Denormalizing the data to convert it back to the original range\n",
    "    return abs((data * (max_coordinate - min_coordinate)) + min_coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array(dataset_json[\"input\"], dtype=np.float32)\n",
    "output_data = np.array(dataset_json[\"output\"], dtype=np.float32)\n",
    "\n",
    "input_data = normalize(input_data)\n",
    "output_data = normalize(output_data)\n",
    "\n",
    "intermediate_steps_num = output_data.shape[1]\n",
    "\n",
    "X_tensor = torch.tensor(input_data, dtype=torch.float, device=device)\n",
    "y_tensor = torch.tensor(output_data, dtype=torch.float, device=device)\n",
    "\n",
    "X_tensor = X_tensor.unsqueeze(1)\n",
    "y_tensor = y_tensor = y_tensor.view(\n",
    "    -1, 2 * intermediate_steps_num\n",
    ")  # Flattening the output tensor, 2 is used because only x, y corrdinates are needed to be predicted for each step\n",
    "\n",
    "\n",
    "del input_data, output_data  # to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(\n",
    "            hidden_dim, 1, bias=False\n",
    "        )  # Attention layer to assign weights to different time steps\n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        scores = self.attn(lstm_out)  # Compute attention scores for each time step\n",
    "        attn_weights = torch.softmax(\n",
    "            scores, dim=1\n",
    "        )  # Apply softmax to normalize attention weights\n",
    "        context = torch.sum(\n",
    "            attn_weights * lstm_out, dim=1\n",
    "        )  # Create context vector by weighted sum of LSTM outputs\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class CursorRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.2):\n",
    "        super(CursorRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=dropout,\n",
    "        )  # LSTM layer for sequence processing\n",
    "        self.attention = Attention(\n",
    "            hidden_dim\n",
    "        )  # Attention mechanism to focus on important time steps\n",
    "        self.residual_fc = nn.Linear(\n",
    "            input_dim, hidden_dim\n",
    "        )  # Residual connection to help with gradient flow\n",
    "        self.layer_norm = nn.LayerNorm(\n",
    "            hidden_dim\n",
    "        )  # Layer normalization for training stability\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_dim, output_dim\n",
    "        )  # Output projection layer to generate final predictions\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # Process sequence through LSTM\n",
    "        context, attn_weights = self.attention(\n",
    "            lstm_out\n",
    "        )  # Apply attention to focus on relevant parts\n",
    "        residual = self.residual_fc(\n",
    "            x[:, -1, :]\n",
    "        )  # Create residual connection from last input\n",
    "        combined = self.layer_norm(\n",
    "            context + residual\n",
    "        )  # Combine attention output with residual and normalize\n",
    "        output = self.fc(combined)  # Generate final trajectory prediction\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_tensor.shape[2]\n",
    "output_size = y_tensor.shape[1]\n",
    "\n",
    "hidden_size = (input_size**2) * int(output_size ** (1 / 2)) * 4\n",
    "epochs = 600\n",
    "lstm_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, num_workers=0, pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/600, Loss: 0.0858934579, LR: 0.0100000000\n",
      "Epoch: 2/600, Loss: 0.0041210859, LR: 0.0100000000\n",
      "Epoch: 3/600, Loss: 0.0038386705, LR: 0.0100000000\n",
      "Epoch: 4/600, Loss: 0.0036732906, LR: 0.0100000000\n",
      "Epoch: 5/600, Loss: 0.0036469724, LR: 0.0100000000\n",
      "Epoch: 6/600, Loss: 0.0036280160, LR: 0.0100000000\n",
      "Epoch: 7/600, Loss: 0.0035917990, LR: 0.0100000000\n",
      "Epoch: 8/600, Loss: 0.0035876005, LR: 0.0100000000\n",
      "Epoch: 9/600, Loss: 0.0036242697, LR: 0.0100000000\n",
      "Epoch: 10/600, Loss: 0.0035910367, LR: 0.0100000000\n",
      "Epoch: 11/600, Loss: 0.0035824694, LR: 0.0100000000\n",
      "Epoch: 12/600, Loss: 0.0035513723, LR: 0.0100000000\n",
      "Epoch: 13/600, Loss: 0.0035476259, LR: 0.0100000000\n",
      "Epoch: 14/600, Loss: 0.0035306382, LR: 0.0100000000\n",
      "Epoch: 15/600, Loss: 0.0035238008, LR: 0.0100000000\n",
      "Epoch: 16/600, Loss: 0.0035208168, LR: 0.0100000000\n",
      "Epoch: 17/600, Loss: 0.0035307485, LR: 0.0100000000\n",
      "Epoch: 18/600, Loss: 0.0035203381, LR: 0.0100000000\n",
      "Epoch: 19/600, Loss: 0.0035132908, LR: 0.0100000000\n",
      "Epoch: 20/600, Loss: 0.0035091560, LR: 0.0100000000\n",
      "Epoch: 21/600, Loss: 0.0035200541, LR: 0.0100000000\n",
      "Epoch: 22/600, Loss: 0.0035315295, LR: 0.0100000000\n",
      "Epoch: 23/600, Loss: 0.0035281724, LR: 0.0100000000\n",
      "Epoch: 24/600, Loss: 0.0035427563, LR: 0.0100000000\n",
      "Epoch: 25/600, Loss: 0.0034942529, LR: 0.0100000000\n",
      "Epoch: 26/600, Loss: 0.0035192184, LR: 0.0100000000\n",
      "Epoch: 27/600, Loss: 0.0035489306, LR: 0.0100000000\n",
      "Epoch: 28/600, Loss: 0.0035069011, LR: 0.0100000000\n",
      "Epoch: 29/600, Loss: 0.0035191081, LR: 0.0100000000\n",
      "Epoch: 30/600, Loss: 0.0035124892, LR: 0.0100000000\n",
      "Epoch: 31/600, Loss: 0.0035202378, LR: 0.0050000000\n",
      "Epoch: 32/600, Loss: 0.0034129171, LR: 0.0050000000\n",
      "Epoch: 33/600, Loss: 0.0034208511, LR: 0.0050000000\n",
      "Epoch: 34/600, Loss: 0.0034200805, LR: 0.0050000000\n",
      "Epoch: 35/600, Loss: 0.0034142269, LR: 0.0050000000\n",
      "Epoch: 36/600, Loss: 0.0034252873, LR: 0.0050000000\n",
      "Epoch: 37/600, Loss: 0.0034201689, LR: 0.0050000000\n",
      "Epoch: 38/600, Loss: 0.0034290326, LR: 0.0025000000\n",
      "Epoch: 39/600, Loss: 0.0033774429, LR: 0.0025000000\n",
      "Epoch: 40/600, Loss: 0.0033720041, LR: 0.0025000000\n",
      "Epoch: 41/600, Loss: 0.0033693972, LR: 0.0025000000\n",
      "Epoch: 42/600, Loss: 0.0033738012, LR: 0.0025000000\n",
      "Epoch: 43/600, Loss: 0.0033720575, LR: 0.0025000000\n",
      "Epoch: 44/600, Loss: 0.0033789825, LR: 0.0025000000\n",
      "Epoch: 45/600, Loss: 0.0033754926, LR: 0.0025000000\n",
      "Epoch: 46/600, Loss: 0.0033760497, LR: 0.0025000000\n",
      "Epoch: 47/600, Loss: 0.0033733619, LR: 0.0012500000\n",
      "Epoch: 48/600, Loss: 0.0033519746, LR: 0.0012500000\n",
      "Epoch: 49/600, Loss: 0.0033443126, LR: 0.0012500000\n",
      "Epoch: 50/600, Loss: 0.0033448698, LR: 0.0012500000\n",
      "Epoch: 51/600, Loss: 0.0033497110, LR: 0.0012500000\n",
      "Epoch: 52/600, Loss: 0.0033484335, LR: 0.0012500000\n",
      "Epoch: 53/600, Loss: 0.0033406819, LR: 0.0012500000\n",
      "Epoch: 54/600, Loss: 0.0033416428, LR: 0.0012500000\n",
      "Epoch: 55/600, Loss: 0.0033416788, LR: 0.0012500000\n",
      "Epoch: 56/600, Loss: 0.0033432674, LR: 0.0012500000\n",
      "Epoch: 57/600, Loss: 0.0033447892, LR: 0.0012500000\n",
      "Epoch: 58/600, Loss: 0.0033405625, LR: 0.0012500000\n",
      "Epoch: 59/600, Loss: 0.0033433578, LR: 0.0006250000\n",
      "Epoch: 60/600, Loss: 0.0033307484, LR: 0.0006250000\n",
      "Epoch: 61/600, Loss: 0.0033291592, LR: 0.0006250000\n",
      "Epoch: 62/600, Loss: 0.0033289230, LR: 0.0006250000\n",
      "Epoch: 63/600, Loss: 0.0033291707, LR: 0.0006250000\n",
      "Epoch: 64/600, Loss: 0.0033268093, LR: 0.0006250000\n",
      "Epoch: 65/600, Loss: 0.0033277782, LR: 0.0006250000\n",
      "Epoch: 66/600, Loss: 0.0033319628, LR: 0.0006250000\n",
      "Epoch: 67/600, Loss: 0.0033284407, LR: 0.0006250000\n",
      "Epoch: 68/600, Loss: 0.0033250671, LR: 0.0006250000\n",
      "Epoch: 69/600, Loss: 0.0033248489, LR: 0.0006250000\n",
      "Epoch: 70/600, Loss: 0.0033215943, LR: 0.0006250000\n",
      "Epoch: 71/600, Loss: 0.0033273336, LR: 0.0006250000\n",
      "Epoch: 72/600, Loss: 0.0033248255, LR: 0.0006250000\n",
      "Epoch: 73/600, Loss: 0.0033253128, LR: 0.0006250000\n",
      "Epoch: 74/600, Loss: 0.0033245033, LR: 0.0006250000\n",
      "Epoch: 75/600, Loss: 0.0033222364, LR: 0.0006250000\n",
      "Epoch: 76/600, Loss: 0.0033226785, LR: 0.0003125000\n",
      "Epoch: 77/600, Loss: 0.0033191889, LR: 0.0003125000\n",
      "Epoch: 78/600, Loss: 0.0033176182, LR: 0.0003125000\n",
      "Epoch: 79/600, Loss: 0.0033200976, LR: 0.0003125000\n",
      "Epoch: 80/600, Loss: 0.0033193071, LR: 0.0003125000\n",
      "Epoch: 81/600, Loss: 0.0033158119, LR: 0.0003125000\n",
      "Epoch: 82/600, Loss: 0.0033156482, LR: 0.0003125000\n",
      "Epoch: 83/600, Loss: 0.0033171559, LR: 0.0003125000\n",
      "Epoch: 84/600, Loss: 0.0033164585, LR: 0.0003125000\n",
      "Epoch: 85/600, Loss: 0.0033246729, LR: 0.0003125000\n",
      "Epoch: 86/600, Loss: 0.0033195373, LR: 0.0003125000\n",
      "Epoch: 87/600, Loss: 0.0033153648, LR: 0.0003125000\n",
      "Epoch: 88/600, Loss: 0.0033182509, LR: 0.0003125000\n",
      "Epoch: 89/600, Loss: 0.0033184379, LR: 0.0003125000\n",
      "Epoch: 90/600, Loss: 0.0033153103, LR: 0.0003125000\n",
      "Epoch: 91/600, Loss: 0.0033108216, LR: 0.0003125000\n",
      "Epoch: 92/600, Loss: 0.0033143141, LR: 0.0003125000\n",
      "Epoch: 93/600, Loss: 0.0033159419, LR: 0.0003125000\n",
      "Epoch: 94/600, Loss: 0.0033160585, LR: 0.0003125000\n",
      "Epoch: 95/600, Loss: 0.0033121969, LR: 0.0003125000\n",
      "Epoch: 96/600, Loss: 0.0033123178, LR: 0.0003125000\n",
      "Epoch: 97/600, Loss: 0.0033116096, LR: 0.0001562500\n",
      "Epoch: 98/600, Loss: 0.0033122183, LR: 0.0001562500\n",
      "Epoch: 99/600, Loss: 0.0033105538, LR: 0.0001562500\n",
      "Epoch: 100/600, Loss: 0.0033104955, LR: 0.0001562500\n",
      "Epoch: 101/600, Loss: 0.0033074915, LR: 0.0001562500\n",
      "Epoch: 102/600, Loss: 0.0033056355, LR: 0.0001562500\n",
      "Epoch: 103/600, Loss: 0.0033066620, LR: 0.0001562500\n",
      "Epoch: 104/600, Loss: 0.0033064345, LR: 0.0001562500\n",
      "Epoch: 105/600, Loss: 0.0033065740, LR: 0.0001562500\n",
      "Epoch: 106/600, Loss: 0.0033091508, LR: 0.0001562500\n",
      "Epoch: 107/600, Loss: 0.0033065821, LR: 0.0001562500\n",
      "Epoch: 108/600, Loss: 0.0033055408, LR: 0.0000781250\n",
      "Epoch: 109/600, Loss: 0.0033054998, LR: 0.0000781250\n",
      "Epoch: 110/600, Loss: 0.0033044401, LR: 0.0000781250\n",
      "Epoch: 111/600, Loss: 0.0033050281, LR: 0.0000781250\n",
      "Epoch: 112/600, Loss: 0.0033051885, LR: 0.0000781250\n",
      "Epoch: 113/600, Loss: 0.0033038148, LR: 0.0000781250\n",
      "Epoch: 114/600, Loss: 0.0033025350, LR: 0.0000781250\n",
      "Epoch: 115/600, Loss: 0.0033029409, LR: 0.0000781250\n",
      "Epoch: 116/600, Loss: 0.0033035772, LR: 0.0000781250\n",
      "Epoch: 117/600, Loss: 0.0033077043, LR: 0.0000781250\n",
      "Epoch: 118/600, Loss: 0.0033021226, LR: 0.0000781250\n",
      "Epoch: 119/600, Loss: 0.0033041428, LR: 0.0000781250\n",
      "Epoch: 120/600, Loss: 0.0032998724, LR: 0.0000781250\n",
      "Epoch: 121/600, Loss: 0.0033037836, LR: 0.0000781250\n",
      "Epoch: 122/600, Loss: 0.0033020247, LR: 0.0000781250\n",
      "Epoch: 123/600, Loss: 0.0032984259, LR: 0.0000781250\n",
      "Epoch: 124/600, Loss: 0.0033050202, LR: 0.0000781250\n",
      "Epoch: 125/600, Loss: 0.0033028917, LR: 0.0000781250\n",
      "Epoch: 126/600, Loss: 0.0032984789, LR: 0.0000781250\n",
      "Epoch: 127/600, Loss: 0.0033032189, LR: 0.0000781250\n",
      "Epoch: 128/600, Loss: 0.0033043953, LR: 0.0000781250\n",
      "Epoch: 129/600, Loss: 0.0033036196, LR: 0.0000390625\n",
      "Epoch: 130/600, Loss: 0.0033004439, LR: 0.0000390625\n",
      "Epoch: 131/600, Loss: 0.0032982446, LR: 0.0000390625\n",
      "Epoch: 132/600, Loss: 0.0033061398, LR: 0.0000390625\n",
      "Epoch: 133/600, Loss: 0.0032991983, LR: 0.0000390625\n",
      "Epoch: 134/600, Loss: 0.0033016493, LR: 0.0000390625\n",
      "Epoch: 135/600, Loss: 0.0032999702, LR: 0.0000195313\n",
      "Epoch: 136/600, Loss: 0.0033021339, LR: 0.0000195313\n",
      "Epoch: 137/600, Loss: 0.0033015762, LR: 0.0000195313\n",
      "Epoch: 138/600, Loss: 0.0033014057, LR: 0.0000195313\n",
      "Epoch: 139/600, Loss: 0.0033001619, LR: 0.0000195313\n",
      "Epoch: 140/600, Loss: 0.0032975370, LR: 0.0000195313\n",
      "Epoch: 141/600, Loss: 0.0033020664, LR: 0.0000195313\n",
      "Epoch: 142/600, Loss: 0.0033003337, LR: 0.0000195313\n",
      "Epoch: 143/600, Loss: 0.0032996901, LR: 0.0000195313\n",
      "Epoch: 144/600, Loss: 0.0033007980, LR: 0.0000195313\n",
      "Epoch: 145/600, Loss: 0.0033014147, LR: 0.0000195313\n",
      "Epoch: 146/600, Loss: 0.0032978846, LR: 0.0000097656\n",
      "Epoch: 147/600, Loss: 0.0032966916, LR: 0.0000097656\n",
      "Epoch: 148/600, Loss: 0.0033001444, LR: 0.0000097656\n",
      "Epoch: 149/600, Loss: 0.0032994478, LR: 0.0000097656\n",
      "Epoch: 150/600, Loss: 0.0033003418, LR: 0.0000097656\n",
      "Epoch: 151/600, Loss: 0.0032972271, LR: 0.0000097656\n",
      "Epoch: 152/600, Loss: 0.0032988304, LR: 0.0000097656\n",
      "Epoch: 153/600, Loss: 0.0033008591, LR: 0.0000048828\n",
      "Epoch: 154/600, Loss: 0.0032999754, LR: 0.0000048828\n",
      "Epoch: 155/600, Loss: 0.0032983525, LR: 0.0000048828\n",
      "Epoch: 156/600, Loss: 0.0032975918, LR: 0.0000048828\n",
      "Epoch: 157/600, Loss: 0.0033009665, LR: 0.0000048828\n",
      "Epoch: 158/600, Loss: 0.0032992182, LR: 0.0000048828\n",
      "Epoch: 159/600, Loss: 0.0032958798, LR: 0.0000048828\n",
      "Epoch: 160/600, Loss: 0.0033005829, LR: 0.0000048828\n",
      "Epoch: 161/600, Loss: 0.0032979799, LR: 0.0000048828\n",
      "Epoch: 162/600, Loss: 0.0033010590, LR: 0.0000048828\n",
      "Epoch: 163/600, Loss: 0.0032988769, LR: 0.0000048828\n",
      "Epoch: 164/600, Loss: 0.0032988349, LR: 0.0000048828\n",
      "Epoch: 165/600, Loss: 0.0032985335, LR: 0.0000024414\n",
      "Epoch: 166/600, Loss: 0.0032987206, LR: 0.0000024414\n",
      "Epoch: 167/600, Loss: 0.0033015305, LR: 0.0000024414\n",
      "Epoch: 168/600, Loss: 0.0032974377, LR: 0.0000024414\n",
      "Epoch: 169/600, Loss: 0.0032986671, LR: 0.0000024414\n",
      "Epoch: 170/600, Loss: 0.0032972781, LR: 0.0000024414\n",
      "Epoch: 171/600, Loss: 0.0033011475, LR: 0.0000012207\n",
      "Epoch: 172/600, Loss: 0.0032963577, LR: 0.0000012207\n",
      "Epoch: 173/600, Loss: 0.0033011058, LR: 0.0000012207\n",
      "Epoch: 174/600, Loss: 0.0032991504, LR: 0.0000012207\n",
      "Epoch: 175/600, Loss: 0.0032970857, LR: 0.0000012207\n",
      "Epoch: 176/600, Loss: 0.0033013657, LR: 0.0000012207\n",
      "Epoch: 177/600, Loss: 0.0032989462, LR: 0.0000006104\n",
      "Epoch: 178/600, Loss: 0.0032995173, LR: 0.0000006104\n",
      "Epoch: 179/600, Loss: 0.0032983557, LR: 0.0000006104\n",
      "Epoch: 180/600, Loss: 0.0032975319, LR: 0.0000006104\n",
      "Epoch: 181/600, Loss: 0.0033000789, LR: 0.0000006104\n",
      "Epoch: 182/600, Loss: 0.0033008944, LR: 0.0000006104\n",
      "Epoch: 183/600, Loss: 0.0032937992, LR: 0.0000006104\n",
      "Epoch: 184/600, Loss: 0.0032996311, LR: 0.0000006104\n",
      "Epoch: 185/600, Loss: 0.0032994305, LR: 0.0000006104\n",
      "Epoch: 186/600, Loss: 0.0033031426, LR: 0.0000006104\n",
      "Epoch: 187/600, Loss: 0.0032960555, LR: 0.0000006104\n",
      "Epoch: 188/600, Loss: 0.0033004759, LR: 0.0000006104\n",
      "Epoch: 189/600, Loss: 0.0033001440, LR: 0.0000003052\n",
      "Epoch: 190/600, Loss: 0.0033022149, LR: 0.0000003052\n",
      "Epoch: 191/600, Loss: 0.0032971375, LR: 0.0000003052\n",
      "Epoch: 192/600, Loss: 0.0032989911, LR: 0.0000003052\n",
      "Epoch: 193/600, Loss: 0.0033008780, LR: 0.0000003052\n",
      "Epoch: 194/600, Loss: 0.0032958590, LR: 0.0000003052\n",
      "Epoch: 195/600, Loss: 0.0033008841, LR: 0.0000001526\n",
      "Epoch: 196/600, Loss: 0.0032993936, LR: 0.0000001526\n",
      "Epoch: 197/600, Loss: 0.0032995901, LR: 0.0000001526\n",
      "Epoch: 198/600, Loss: 0.0032968960, LR: 0.0000001526\n",
      "Epoch: 199/600, Loss: 0.0032988570, LR: 0.0000001526\n",
      "Epoch: 200/600, Loss: 0.0033012811, LR: 0.0000001526\n",
      "Epoch: 201/600, Loss: 0.0032976559, LR: 0.0000000763\n",
      "Epoch: 202/600, Loss: 0.0032969099, LR: 0.0000000763\n",
      "Epoch: 203/600, Loss: 0.0032987047, LR: 0.0000000763\n",
      "Epoch: 204/600, Loss: 0.0032955821, LR: 0.0000000763\n",
      "Epoch: 205/600, Loss: 0.0032985375, LR: 0.0000000763\n",
      "Epoch: 206/600, Loss: 0.0033000611, LR: 0.0000000763\n",
      "Epoch: 207/600, Loss: 0.0032986474, LR: 0.0000000381\n",
      "Epoch: 208/600, Loss: 0.0033007917, LR: 0.0000000381\n",
      "Epoch: 209/600, Loss: 0.0032999802, LR: 0.0000000381\n",
      "Epoch: 210/600, Loss: 0.0032976711, LR: 0.0000000381\n",
      "Epoch: 211/600, Loss: 0.0032971338, LR: 0.0000000381\n",
      "Epoch: 212/600, Loss: 0.0032988628, LR: 0.0000000381\n",
      "Epoch: 213/600, Loss: 0.0032994463, LR: 0.0000000191\n",
      "Epoch: 214/600, Loss: 0.0032998300, LR: 0.0000000191\n",
      "Epoch: 215/600, Loss: 0.0032991435, LR: 0.0000000191\n",
      "Epoch: 216/600, Loss: 0.0032983488, LR: 0.0000000191\n",
      "Epoch: 217/600, Loss: 0.0033007208, LR: 0.0000000191\n",
      "Epoch: 218/600, Loss: 0.0032982194, LR: 0.0000000191\n",
      "Epoch: 219/600, Loss: 0.0032971271, LR: 0.0000000191\n",
      "Epoch: 220/600, Loss: 0.0032992337, LR: 0.0000000191\n",
      "Epoch: 221/600, Loss: 0.0032956524, LR: 0.0000000191\n",
      "Epoch: 222/600, Loss: 0.0032966031, LR: 0.0000000191\n",
      "Epoch: 223/600, Loss: 0.0032979993, LR: 0.0000000191\n",
      "Epoch: 224/600, Loss: 0.0032971220, LR: 0.0000000191\n",
      "Epoch: 225/600, Loss: 0.0032961358, LR: 0.0000000191\n",
      "Epoch: 226/600, Loss: 0.0032978428, LR: 0.0000000191\n",
      "Epoch: 227/600, Loss: 0.0033000268, LR: 0.0000000191\n",
      "Epoch: 228/600, Loss: 0.0032973551, LR: 0.0000000191\n",
      "Epoch: 229/600, Loss: 0.0032958515, LR: 0.0000000191\n",
      "Epoch: 230/600, Loss: 0.0032971672, LR: 0.0000000191\n",
      "Epoch: 231/600, Loss: 0.0032978727, LR: 0.0000000191\n",
      "Epoch: 232/600, Loss: 0.0032989380, LR: 0.0000000191\n",
      "Epoch: 233/600, Loss: 0.0032936599, LR: 0.0000000191\n",
      "Epoch: 234/600, Loss: 0.0032998639, LR: 0.0000000191\n",
      "Epoch: 235/600, Loss: 0.0032980717, LR: 0.0000000191\n",
      "Epoch: 236/600, Loss: 0.0032975724, LR: 0.0000000191\n",
      "Epoch: 237/600, Loss: 0.0032978636, LR: 0.0000000191\n",
      "Epoch: 238/600, Loss: 0.0032986459, LR: 0.0000000191\n",
      "Epoch: 239/600, Loss: 0.0033004620, LR: 0.0000000191\n",
      "Epoch: 240/600, Loss: 0.0032985039, LR: 0.0000000191\n",
      "Epoch: 241/600, Loss: 0.0032973918, LR: 0.0000000191\n",
      "Epoch: 242/600, Loss: 0.0032980907, LR: 0.0000000191\n",
      "Epoch: 243/600, Loss: 0.0032978925, LR: 0.0000000191\n",
      "Epoch: 244/600, Loss: 0.0032991130, LR: 0.0000000191\n",
      "Epoch: 245/600, Loss: 0.0032964895, LR: 0.0000000191\n",
      "Epoch: 246/600, Loss: 0.0033019060, LR: 0.0000000191\n",
      "Epoch: 247/600, Loss: 0.0032991353, LR: 0.0000000191\n",
      "Epoch: 248/600, Loss: 0.0032996019, LR: 0.0000000191\n",
      "Epoch: 249/600, Loss: 0.0032966787, LR: 0.0000000191\n",
      "Epoch: 250/600, Loss: 0.0032955188, LR: 0.0000000191\n",
      "Epoch: 251/600, Loss: 0.0032990427, LR: 0.0000000191\n",
      "Epoch: 252/600, Loss: 0.0032991375, LR: 0.0000000191\n",
      "Epoch: 253/600, Loss: 0.0033008672, LR: 0.0000000191\n",
      "Epoch: 254/600, Loss: 0.0032972212, LR: 0.0000000191\n",
      "Epoch: 255/600, Loss: 0.0032991365, LR: 0.0000000191\n",
      "Epoch: 256/600, Loss: 0.0032965027, LR: 0.0000000191\n",
      "Epoch: 257/600, Loss: 0.0032954443, LR: 0.0000000191\n",
      "Epoch: 258/600, Loss: 0.0032976027, LR: 0.0000000191\n",
      "Epoch: 259/600, Loss: 0.0033002873, LR: 0.0000000191\n",
      "Epoch: 260/600, Loss: 0.0032980646, LR: 0.0000000191\n",
      "Epoch: 261/600, Loss: 0.0032998085, LR: 0.0000000191\n",
      "Epoch: 262/600, Loss: 0.0033021603, LR: 0.0000000191\n",
      "Epoch: 263/600, Loss: 0.0032967020, LR: 0.0000000191\n",
      "Epoch: 264/600, Loss: 0.0032966230, LR: 0.0000000191\n",
      "Epoch: 265/600, Loss: 0.0033001056, LR: 0.0000000191\n",
      "Epoch: 266/600, Loss: 0.0033006091, LR: 0.0000000191\n",
      "Epoch: 267/600, Loss: 0.0033017552, LR: 0.0000000191\n",
      "Epoch: 268/600, Loss: 0.0032938296, LR: 0.0000000191\n",
      "Epoch: 269/600, Loss: 0.0033016739, LR: 0.0000000191\n",
      "Epoch: 270/600, Loss: 0.0032991640, LR: 0.0000000191\n",
      "Epoch: 271/600, Loss: 0.0032964524, LR: 0.0000000191\n",
      "Epoch: 272/600, Loss: 0.0033010449, LR: 0.0000000191\n",
      "Epoch: 273/600, Loss: 0.0032999271, LR: 0.0000000191\n",
      "Epoch: 274/600, Loss: 0.0032988304, LR: 0.0000000191\n",
      "Epoch: 275/600, Loss: 0.0032992279, LR: 0.0000000191\n",
      "Epoch: 276/600, Loss: 0.0032977393, LR: 0.0000000191\n",
      "Epoch: 277/600, Loss: 0.0032987580, LR: 0.0000000191\n",
      "Epoch: 278/600, Loss: 0.0032996068, LR: 0.0000000191\n",
      "Epoch: 279/600, Loss: 0.0032983938, LR: 0.0000000191\n",
      "Epoch: 280/600, Loss: 0.0032961770, LR: 0.0000000191\n",
      "Epoch: 281/600, Loss: 0.0032949385, LR: 0.0000000191\n",
      "Epoch: 282/600, Loss: 0.0032957719, LR: 0.0000000191\n",
      "Epoch: 283/600, Loss: 0.0032973082, LR: 0.0000000191\n",
      "Epoch: 284/600, Loss: 0.0032986929, LR: 0.0000000191\n",
      "Epoch: 285/600, Loss: 0.0032960005, LR: 0.0000000191\n",
      "Epoch: 286/600, Loss: 0.0033031087, LR: 0.0000000191\n",
      "Epoch: 287/600, Loss: 0.0032968353, LR: 0.0000000191\n",
      "Epoch: 288/600, Loss: 0.0032993569, LR: 0.0000000191\n",
      "Epoch: 289/600, Loss: 0.0032970122, LR: 0.0000000191\n",
      "Epoch: 290/600, Loss: 0.0032998414, LR: 0.0000000191\n",
      "Epoch: 291/600, Loss: 0.0032967849, LR: 0.0000000191\n",
      "Epoch: 292/600, Loss: 0.0032977973, LR: 0.0000000191\n",
      "Epoch: 293/600, Loss: 0.0032991995, LR: 0.0000000191\n",
      "Epoch: 294/600, Loss: 0.0033027732, LR: 0.0000000191\n",
      "Epoch: 295/600, Loss: 0.0033015107, LR: 0.0000000191\n",
      "Epoch: 296/600, Loss: 0.0032990628, LR: 0.0000000191\n",
      "Epoch: 297/600, Loss: 0.0033019472, LR: 0.0000000191\n",
      "Epoch: 298/600, Loss: 0.0032995343, LR: 0.0000000191\n",
      "Epoch: 299/600, Loss: 0.0033004617, LR: 0.0000000191\n",
      "Epoch: 300/600, Loss: 0.0032940856, LR: 0.0000000191\n",
      "Epoch: 301/600, Loss: 0.0033004657, LR: 0.0000000191\n",
      "Epoch: 302/600, Loss: 0.0032977734, LR: 0.0000000191\n",
      "Epoch: 303/600, Loss: 0.0032967916, LR: 0.0000000191\n",
      "Epoch: 304/600, Loss: 0.0032961911, LR: 0.0000000191\n",
      "Epoch: 305/600, Loss: 0.0032988979, LR: 0.0000000191\n",
      "Epoch: 306/600, Loss: 0.0032973701, LR: 0.0000000191\n",
      "Epoch: 307/600, Loss: 0.0032973818, LR: 0.0000000191\n",
      "Epoch: 308/600, Loss: 0.0032965762, LR: 0.0000000191\n",
      "Epoch: 309/600, Loss: 0.0033012424, LR: 0.0000000191\n",
      "Epoch: 310/600, Loss: 0.0032977937, LR: 0.0000000191\n",
      "Epoch: 311/600, Loss: 0.0032980780, LR: 0.0000000191\n",
      "Epoch: 312/600, Loss: 0.0033010548, LR: 0.0000000191\n",
      "Epoch: 313/600, Loss: 0.0033020613, LR: 0.0000000191\n",
      "Epoch: 314/600, Loss: 0.0032969696, LR: 0.0000000191\n",
      "Epoch: 315/600, Loss: 0.0032998589, LR: 0.0000000191\n",
      "Epoch: 316/600, Loss: 0.0033006254, LR: 0.0000000191\n",
      "Epoch: 317/600, Loss: 0.0033001411, LR: 0.0000000191\n",
      "Epoch: 318/600, Loss: 0.0032962823, LR: 0.0000000191\n",
      "Epoch: 319/600, Loss: 0.0032981392, LR: 0.0000000191\n",
      "Epoch: 320/600, Loss: 0.0033003652, LR: 0.0000000191\n",
      "Epoch: 321/600, Loss: 0.0032997479, LR: 0.0000000191\n",
      "Epoch: 322/600, Loss: 0.0032971120, LR: 0.0000000191\n",
      "Epoch: 323/600, Loss: 0.0033008997, LR: 0.0000000191\n",
      "Epoch: 324/600, Loss: 0.0032993279, LR: 0.0000000191\n",
      "Epoch: 325/600, Loss: 0.0033003929, LR: 0.0000000191\n",
      "Epoch: 326/600, Loss: 0.0032978164, LR: 0.0000000191\n",
      "Epoch: 327/600, Loss: 0.0032966272, LR: 0.0000000191\n",
      "Epoch: 328/600, Loss: 0.0032983622, LR: 0.0000000191\n",
      "Epoch: 329/600, Loss: 0.0032987263, LR: 0.0000000191\n",
      "Epoch: 330/600, Loss: 0.0032993063, LR: 0.0000000191\n",
      "Epoch: 331/600, Loss: 0.0032974685, LR: 0.0000000191\n",
      "Epoch: 332/600, Loss: 0.0032980016, LR: 0.0000000191\n",
      "Epoch: 333/600, Loss: 0.0032977321, LR: 0.0000000191\n",
      "Epoch: 334/600, Loss: 0.0032972458, LR: 0.0000000191\n",
      "Epoch: 335/600, Loss: 0.0032990472, LR: 0.0000000191\n",
      "Epoch: 336/600, Loss: 0.0032963217, LR: 0.0000000191\n",
      "Epoch: 337/600, Loss: 0.0032998399, LR: 0.0000000191\n",
      "Epoch: 338/600, Loss: 0.0032990288, LR: 0.0000000191\n",
      "Epoch: 339/600, Loss: 0.0032985409, LR: 0.0000000191\n",
      "Epoch: 340/600, Loss: 0.0033003223, LR: 0.0000000191\n",
      "Epoch: 341/600, Loss: 0.0033026808, LR: 0.0000000191\n",
      "Epoch: 342/600, Loss: 0.0033023950, LR: 0.0000000191\n",
      "Epoch: 343/600, Loss: 0.0032983897, LR: 0.0000000191\n",
      "Epoch: 344/600, Loss: 0.0032999929, LR: 0.0000000191\n",
      "Epoch: 345/600, Loss: 0.0032972140, LR: 0.0000000191\n",
      "Epoch: 346/600, Loss: 0.0032999988, LR: 0.0000000191\n",
      "Epoch: 347/600, Loss: 0.0032961821, LR: 0.0000000191\n",
      "Epoch: 348/600, Loss: 0.0032970134, LR: 0.0000000191\n",
      "Epoch: 349/600, Loss: 0.0032964271, LR: 0.0000000191\n",
      "Epoch: 350/600, Loss: 0.0032978330, LR: 0.0000000191\n",
      "Epoch: 351/600, Loss: 0.0032974130, LR: 0.0000000191\n",
      "Epoch: 352/600, Loss: 0.0032984543, LR: 0.0000000191\n",
      "Epoch: 353/600, Loss: 0.0032994452, LR: 0.0000000191\n",
      "Epoch: 354/600, Loss: 0.0033000512, LR: 0.0000000191\n",
      "Epoch: 355/600, Loss: 0.0032979502, LR: 0.0000000191\n",
      "Epoch: 356/600, Loss: 0.0032976000, LR: 0.0000000191\n",
      "Epoch: 357/600, Loss: 0.0032988035, LR: 0.0000000191\n",
      "Epoch: 358/600, Loss: 0.0032997928, LR: 0.0000000191\n",
      "Epoch: 359/600, Loss: 0.0033003696, LR: 0.0000000191\n",
      "Epoch: 360/600, Loss: 0.0032972534, LR: 0.0000000191\n",
      "Epoch: 361/600, Loss: 0.0032951850, LR: 0.0000000191\n",
      "Epoch: 362/600, Loss: 0.0032976405, LR: 0.0000000191\n",
      "Epoch: 363/600, Loss: 0.0033021446, LR: 0.0000000191\n",
      "Epoch: 364/600, Loss: 0.0032986937, LR: 0.0000000191\n",
      "Epoch: 365/600, Loss: 0.0032986226, LR: 0.0000000191\n",
      "Epoch: 366/600, Loss: 0.0032989130, LR: 0.0000000191\n",
      "Epoch: 367/600, Loss: 0.0032986617, LR: 0.0000000191\n",
      "Epoch: 368/600, Loss: 0.0032997077, LR: 0.0000000191\n",
      "Epoch: 369/600, Loss: 0.0032969258, LR: 0.0000000191\n",
      "Epoch: 370/600, Loss: 0.0032995156, LR: 0.0000000191\n",
      "Epoch: 371/600, Loss: 0.0032980178, LR: 0.0000000191\n",
      "Epoch: 372/600, Loss: 0.0032994982, LR: 0.0000000191\n",
      "Epoch: 373/600, Loss: 0.0032986014, LR: 0.0000000191\n",
      "Epoch: 374/600, Loss: 0.0032954150, LR: 0.0000000191\n",
      "Epoch: 375/600, Loss: 0.0032959291, LR: 0.0000000191\n",
      "Epoch: 376/600, Loss: 0.0033015415, LR: 0.0000000191\n",
      "Epoch: 377/600, Loss: 0.0033015707, LR: 0.0000000191\n",
      "Epoch: 378/600, Loss: 0.0032985928, LR: 0.0000000191\n",
      "Epoch: 379/600, Loss: 0.0033010821, LR: 0.0000000191\n",
      "Epoch: 380/600, Loss: 0.0032965660, LR: 0.0000000191\n",
      "Epoch: 381/600, Loss: 0.0032981250, LR: 0.0000000191\n",
      "Epoch: 382/600, Loss: 0.0032964121, LR: 0.0000000191\n",
      "Epoch: 383/600, Loss: 0.0033026220, LR: 0.0000000191\n",
      "Epoch: 384/600, Loss: 0.0032963715, LR: 0.0000000191\n",
      "Epoch: 385/600, Loss: 0.0032976088, LR: 0.0000000191\n",
      "Epoch: 386/600, Loss: 0.0032975734, LR: 0.0000000191\n",
      "Epoch: 387/600, Loss: 0.0032993683, LR: 0.0000000191\n",
      "Epoch: 388/600, Loss: 0.0033006041, LR: 0.0000000191\n",
      "Epoch: 389/600, Loss: 0.0033017476, LR: 0.0000000191\n",
      "Epoch: 390/600, Loss: 0.0032951267, LR: 0.0000000191\n",
      "Epoch: 391/600, Loss: 0.0032964945, LR: 0.0000000191\n",
      "Epoch: 392/600, Loss: 0.0032987186, LR: 0.0000000191\n",
      "Epoch: 393/600, Loss: 0.0033003397, LR: 0.0000000191\n",
      "Epoch: 394/600, Loss: 0.0033003988, LR: 0.0000000191\n",
      "Epoch: 395/600, Loss: 0.0032962197, LR: 0.0000000191\n",
      "Epoch: 396/600, Loss: 0.0032975311, LR: 0.0000000191\n",
      "Epoch: 397/600, Loss: 0.0032983468, LR: 0.0000000191\n",
      "Epoch: 398/600, Loss: 0.0032965928, LR: 0.0000000191\n",
      "Epoch: 399/600, Loss: 0.0032993836, LR: 0.0000000191\n",
      "Epoch: 400/600, Loss: 0.0032961214, LR: 0.0000000191\n",
      "Epoch: 401/600, Loss: 0.0032986838, LR: 0.0000000191\n",
      "Epoch: 402/600, Loss: 0.0032960420, LR: 0.0000000191\n",
      "Epoch: 403/600, Loss: 0.0033000838, LR: 0.0000000191\n",
      "Epoch: 404/600, Loss: 0.0033009902, LR: 0.0000000191\n",
      "Epoch: 405/600, Loss: 0.0032981894, LR: 0.0000000191\n",
      "Epoch: 406/600, Loss: 0.0033004735, LR: 0.0000000191\n",
      "Epoch: 407/600, Loss: 0.0032991808, LR: 0.0000000191\n",
      "Epoch: 408/600, Loss: 0.0032973882, LR: 0.0000000191\n",
      "Epoch: 409/600, Loss: 0.0032980003, LR: 0.0000000191\n",
      "Epoch: 410/600, Loss: 0.0032974393, LR: 0.0000000191\n",
      "Epoch: 411/600, Loss: 0.0032960399, LR: 0.0000000191\n",
      "Epoch: 412/600, Loss: 0.0033000264, LR: 0.0000000191\n",
      "Epoch: 413/600, Loss: 0.0032984230, LR: 0.0000000191\n",
      "Epoch: 414/600, Loss: 0.0032990762, LR: 0.0000000191\n",
      "Epoch: 415/600, Loss: 0.0032984501, LR: 0.0000000191\n",
      "Epoch: 416/600, Loss: 0.0032993195, LR: 0.0000000191\n",
      "Epoch: 417/600, Loss: 0.0032997326, LR: 0.0000000191\n",
      "Epoch: 418/600, Loss: 0.0032965198, LR: 0.0000000191\n",
      "Epoch: 419/600, Loss: 0.0032989366, LR: 0.0000000191\n",
      "Epoch: 420/600, Loss: 0.0033005941, LR: 0.0000000191\n",
      "Epoch: 421/600, Loss: 0.0033002102, LR: 0.0000000191\n",
      "Epoch: 422/600, Loss: 0.0032976431, LR: 0.0000000191\n",
      "Epoch: 423/600, Loss: 0.0033009339, LR: 0.0000000191\n",
      "Epoch: 424/600, Loss: 0.0032993268, LR: 0.0000000191\n",
      "Epoch: 425/600, Loss: 0.0032943570, LR: 0.0000000191\n",
      "Epoch: 426/600, Loss: 0.0033006819, LR: 0.0000000191\n",
      "Epoch: 427/600, Loss: 0.0032980161, LR: 0.0000000191\n",
      "Epoch: 428/600, Loss: 0.0032949658, LR: 0.0000000191\n",
      "Epoch: 429/600, Loss: 0.0033011111, LR: 0.0000000191\n",
      "Epoch: 430/600, Loss: 0.0032963962, LR: 0.0000000191\n",
      "Epoch: 431/600, Loss: 0.0032960005, LR: 0.0000000191\n",
      "Epoch: 432/600, Loss: 0.0032974476, LR: 0.0000000191\n",
      "Epoch: 433/600, Loss: 0.0032973404, LR: 0.0000000191\n",
      "Epoch: 434/600, Loss: 0.0032971763, LR: 0.0000000191\n",
      "Epoch: 435/600, Loss: 0.0033017386, LR: 0.0000000191\n",
      "Epoch: 436/600, Loss: 0.0032984173, LR: 0.0000000191\n",
      "Epoch: 437/600, Loss: 0.0032984527, LR: 0.0000000191\n",
      "Epoch: 438/600, Loss: 0.0033010841, LR: 0.0000000191\n",
      "Epoch: 439/600, Loss: 0.0033004963, LR: 0.0000000191\n",
      "Epoch: 440/600, Loss: 0.0033003770, LR: 0.0000000191\n",
      "Epoch: 441/600, Loss: 0.0033036719, LR: 0.0000000191\n",
      "Epoch: 442/600, Loss: 0.0032981322, LR: 0.0000000191\n",
      "Epoch: 443/600, Loss: 0.0032983656, LR: 0.0000000191\n",
      "Epoch: 444/600, Loss: 0.0032957125, LR: 0.0000000191\n",
      "Epoch: 445/600, Loss: 0.0032964617, LR: 0.0000000191\n",
      "Epoch: 446/600, Loss: 0.0032993767, LR: 0.0000000191\n",
      "Epoch: 447/600, Loss: 0.0032989052, LR: 0.0000000191\n",
      "Epoch: 448/600, Loss: 0.0032966793, LR: 0.0000000191\n",
      "Epoch: 449/600, Loss: 0.0033008382, LR: 0.0000000191\n",
      "Epoch: 450/600, Loss: 0.0032990669, LR: 0.0000000191\n",
      "Epoch: 451/600, Loss: 0.0033003806, LR: 0.0000000191\n",
      "Epoch: 452/600, Loss: 0.0032988932, LR: 0.0000000191\n",
      "Epoch: 453/600, Loss: 0.0032966944, LR: 0.0000000191\n",
      "Epoch: 454/600, Loss: 0.0032997984, LR: 0.0000000191\n",
      "Epoch: 455/600, Loss: 0.0032971285, LR: 0.0000000191\n",
      "Epoch: 456/600, Loss: 0.0033021051, LR: 0.0000000191\n",
      "Epoch: 457/600, Loss: 0.0032998371, LR: 0.0000000191\n",
      "Epoch: 458/600, Loss: 0.0033013290, LR: 0.0000000191\n",
      "Epoch: 459/600, Loss: 0.0033018773, LR: 0.0000000191\n",
      "Epoch: 460/600, Loss: 0.0032963178, LR: 0.0000000191\n",
      "Epoch: 461/600, Loss: 0.0033018768, LR: 0.0000000191\n",
      "Epoch: 462/600, Loss: 0.0032942168, LR: 0.0000000191\n",
      "Epoch: 463/600, Loss: 0.0032986270, LR: 0.0000000191\n",
      "Epoch: 464/600, Loss: 0.0032981707, LR: 0.0000000191\n",
      "Epoch: 465/600, Loss: 0.0033005679, LR: 0.0000000191\n",
      "Epoch: 466/600, Loss: 0.0032961738, LR: 0.0000000191\n",
      "Epoch: 467/600, Loss: 0.0032964236, LR: 0.0000000191\n",
      "Epoch: 468/600, Loss: 0.0032975093, LR: 0.0000000191\n",
      "Epoch: 469/600, Loss: 0.0032960012, LR: 0.0000000191\n",
      "Epoch: 470/600, Loss: 0.0032985474, LR: 0.0000000191\n",
      "Epoch: 471/600, Loss: 0.0032985234, LR: 0.0000000191\n",
      "Epoch: 472/600, Loss: 0.0032974001, LR: 0.0000000191\n",
      "Epoch: 473/600, Loss: 0.0032973440, LR: 0.0000000191\n",
      "Epoch: 474/600, Loss: 0.0032993203, LR: 0.0000000191\n",
      "Epoch: 475/600, Loss: 0.0032969687, LR: 0.0000000191\n",
      "Epoch: 476/600, Loss: 0.0032966221, LR: 0.0000000191\n",
      "Epoch: 477/600, Loss: 0.0032956540, LR: 0.0000000191\n",
      "Epoch: 478/600, Loss: 0.0032977319, LR: 0.0000000191\n",
      "Epoch: 479/600, Loss: 0.0032993031, LR: 0.0000000191\n",
      "Epoch: 480/600, Loss: 0.0032992544, LR: 0.0000000191\n",
      "Epoch: 481/600, Loss: 0.0032972899, LR: 0.0000000191\n",
      "Epoch: 482/600, Loss: 0.0033016130, LR: 0.0000000191\n",
      "Epoch: 483/600, Loss: 0.0032999323, LR: 0.0000000191\n",
      "Epoch: 484/600, Loss: 0.0033001177, LR: 0.0000000191\n",
      "Epoch: 485/600, Loss: 0.0032981481, LR: 0.0000000191\n",
      "Epoch: 486/600, Loss: 0.0032985057, LR: 0.0000000191\n",
      "Epoch: 487/600, Loss: 0.0033014925, LR: 0.0000000191\n",
      "Epoch: 488/600, Loss: 0.0032970991, LR: 0.0000000191\n",
      "Epoch: 489/600, Loss: 0.0032980306, LR: 0.0000000191\n",
      "Epoch: 490/600, Loss: 0.0033021043, LR: 0.0000000191\n",
      "Epoch: 491/600, Loss: 0.0032991419, LR: 0.0000000191\n",
      "Epoch: 492/600, Loss: 0.0032993185, LR: 0.0000000191\n",
      "Epoch: 493/600, Loss: 0.0033013396, LR: 0.0000000191\n",
      "Epoch: 494/600, Loss: 0.0032987115, LR: 0.0000000191\n",
      "Epoch: 495/600, Loss: 0.0032979641, LR: 0.0000000191\n",
      "Epoch: 496/600, Loss: 0.0032987609, LR: 0.0000000191\n",
      "Epoch: 497/600, Loss: 0.0032972859, LR: 0.0000000191\n",
      "Epoch: 498/600, Loss: 0.0032995483, LR: 0.0000000191\n",
      "Epoch: 499/600, Loss: 0.0032989106, LR: 0.0000000191\n",
      "Epoch: 500/600, Loss: 0.0032992746, LR: 0.0000000191\n",
      "Epoch: 501/600, Loss: 0.0032968654, LR: 0.0000000191\n",
      "Epoch: 502/600, Loss: 0.0032966386, LR: 0.0000000191\n",
      "Epoch: 503/600, Loss: 0.0033001128, LR: 0.0000000191\n",
      "Epoch: 504/600, Loss: 0.0032987522, LR: 0.0000000191\n",
      "Epoch: 505/600, Loss: 0.0033011535, LR: 0.0000000191\n",
      "Epoch: 506/600, Loss: 0.0032971213, LR: 0.0000000191\n",
      "Epoch: 507/600, Loss: 0.0032952642, LR: 0.0000000191\n",
      "Epoch: 508/600, Loss: 0.0033005035, LR: 0.0000000191\n",
      "Epoch: 509/600, Loss: 0.0032998000, LR: 0.0000000191\n",
      "Epoch: 510/600, Loss: 0.0032970968, LR: 0.0000000191\n",
      "Epoch: 511/600, Loss: 0.0032987497, LR: 0.0000000191\n",
      "Epoch: 512/600, Loss: 0.0032985916, LR: 0.0000000191\n",
      "Epoch: 513/600, Loss: 0.0033001822, LR: 0.0000000191\n",
      "Epoch: 514/600, Loss: 0.0033013099, LR: 0.0000000191\n",
      "Epoch: 515/600, Loss: 0.0032974882, LR: 0.0000000191\n",
      "Epoch: 516/600, Loss: 0.0032974174, LR: 0.0000000191\n",
      "Epoch: 517/600, Loss: 0.0032953396, LR: 0.0000000191\n",
      "Epoch: 518/600, Loss: 0.0032989983, LR: 0.0000000191\n",
      "Epoch: 519/600, Loss: 0.0032982704, LR: 0.0000000191\n",
      "Epoch: 520/600, Loss: 0.0032999113, LR: 0.0000000191\n",
      "Epoch: 521/600, Loss: 0.0033003242, LR: 0.0000000191\n",
      "Epoch: 522/600, Loss: 0.0032978350, LR: 0.0000000191\n",
      "Epoch: 523/600, Loss: 0.0032955121, LR: 0.0000000191\n",
      "Epoch: 524/600, Loss: 0.0032954358, LR: 0.0000000191\n",
      "Epoch: 525/600, Loss: 0.0032986873, LR: 0.0000000191\n",
      "Epoch: 526/600, Loss: 0.0032969958, LR: 0.0000000191\n",
      "Epoch: 527/600, Loss: 0.0032984020, LR: 0.0000000191\n",
      "Epoch: 528/600, Loss: 0.0032989920, LR: 0.0000000191\n",
      "Epoch: 529/600, Loss: 0.0033008734, LR: 0.0000000191\n",
      "Epoch: 530/600, Loss: 0.0032972013, LR: 0.0000000191\n",
      "Epoch: 531/600, Loss: 0.0033016343, LR: 0.0000000191\n",
      "Epoch: 532/600, Loss: 0.0032997034, LR: 0.0000000191\n",
      "Epoch: 533/600, Loss: 0.0032991515, LR: 0.0000000191\n",
      "Epoch: 534/600, Loss: 0.0032988213, LR: 0.0000000191\n",
      "Epoch: 535/600, Loss: 0.0032955507, LR: 0.0000000191\n",
      "Epoch: 536/600, Loss: 0.0032950342, LR: 0.0000000191\n",
      "Epoch: 537/600, Loss: 0.0032996821, LR: 0.0000000191\n",
      "Epoch: 538/600, Loss: 0.0032989618, LR: 0.0000000191\n",
      "Epoch: 539/600, Loss: 0.0033004635, LR: 0.0000000191\n",
      "Epoch: 540/600, Loss: 0.0032974744, LR: 0.0000000191\n",
      "Epoch: 541/600, Loss: 0.0032965853, LR: 0.0000000191\n",
      "Epoch: 542/600, Loss: 0.0032999727, LR: 0.0000000191\n",
      "Epoch: 543/600, Loss: 0.0033026640, LR: 0.0000000191\n",
      "Epoch: 544/600, Loss: 0.0032998670, LR: 0.0000000191\n",
      "Epoch: 545/600, Loss: 0.0032993325, LR: 0.0000000191\n",
      "Epoch: 546/600, Loss: 0.0032985479, LR: 0.0000000191\n",
      "Epoch: 547/600, Loss: 0.0032981500, LR: 0.0000000191\n",
      "Epoch: 548/600, Loss: 0.0032989204, LR: 0.0000000191\n",
      "Epoch: 549/600, Loss: 0.0032988764, LR: 0.0000000191\n",
      "Epoch: 550/600, Loss: 0.0032982972, LR: 0.0000000191\n",
      "Epoch: 551/600, Loss: 0.0032997480, LR: 0.0000000191\n",
      "Epoch: 552/600, Loss: 0.0032966229, LR: 0.0000000191\n",
      "Epoch: 553/600, Loss: 0.0032995223, LR: 0.0000000191\n",
      "Epoch: 554/600, Loss: 0.0032948210, LR: 0.0000000191\n",
      "Epoch: 555/600, Loss: 0.0032995399, LR: 0.0000000191\n",
      "Epoch: 556/600, Loss: 0.0032954818, LR: 0.0000000191\n",
      "Epoch: 557/600, Loss: 0.0032974677, LR: 0.0000000191\n",
      "Epoch: 558/600, Loss: 0.0032953525, LR: 0.0000000191\n",
      "Epoch: 559/600, Loss: 0.0032972180, LR: 0.0000000191\n",
      "Epoch: 560/600, Loss: 0.0032991647, LR: 0.0000000191\n",
      "Epoch: 561/600, Loss: 0.0032982520, LR: 0.0000000191\n",
      "Epoch: 562/600, Loss: 0.0032982592, LR: 0.0000000191\n",
      "Epoch: 563/600, Loss: 0.0032980959, LR: 0.0000000191\n",
      "Epoch: 564/600, Loss: 0.0033032841, LR: 0.0000000191\n",
      "Epoch: 565/600, Loss: 0.0032996100, LR: 0.0000000191\n",
      "Epoch: 566/600, Loss: 0.0032975003, LR: 0.0000000191\n",
      "Epoch: 567/600, Loss: 0.0032978819, LR: 0.0000000191\n",
      "Epoch: 568/600, Loss: 0.0032998248, LR: 0.0000000191\n",
      "Epoch: 569/600, Loss: 0.0032987629, LR: 0.0000000191\n",
      "Epoch: 570/600, Loss: 0.0032986672, LR: 0.0000000191\n",
      "Epoch: 571/600, Loss: 0.0033001156, LR: 0.0000000191\n",
      "Epoch: 572/600, Loss: 0.0032966709, LR: 0.0000000191\n",
      "Epoch: 573/600, Loss: 0.0032989937, LR: 0.0000000191\n",
      "Epoch: 574/600, Loss: 0.0033000355, LR: 0.0000000191\n",
      "Epoch: 575/600, Loss: 0.0032965810, LR: 0.0000000191\n",
      "Epoch: 576/600, Loss: 0.0032986850, LR: 0.0000000191\n",
      "Epoch: 577/600, Loss: 0.0032971331, LR: 0.0000000191\n",
      "Epoch: 578/600, Loss: 0.0032990506, LR: 0.0000000191\n",
      "Epoch: 579/600, Loss: 0.0032970750, LR: 0.0000000191\n",
      "Epoch: 580/600, Loss: 0.0032940307, LR: 0.0000000191\n",
      "Epoch: 581/600, Loss: 0.0032983509, LR: 0.0000000191\n",
      "Epoch: 582/600, Loss: 0.0032998571, LR: 0.0000000191\n",
      "Epoch: 583/600, Loss: 0.0032995341, LR: 0.0000000191\n",
      "Epoch: 584/600, Loss: 0.0033005355, LR: 0.0000000191\n",
      "Epoch: 585/600, Loss: 0.0032948723, LR: 0.0000000191\n",
      "Epoch: 586/600, Loss: 0.0032980710, LR: 0.0000000191\n",
      "Epoch: 587/600, Loss: 0.0032981660, LR: 0.0000000191\n",
      "Epoch: 588/600, Loss: 0.0032964830, LR: 0.0000000191\n",
      "Epoch: 589/600, Loss: 0.0032962566, LR: 0.0000000191\n",
      "Epoch: 590/600, Loss: 0.0032994184, LR: 0.0000000191\n",
      "Epoch: 591/600, Loss: 0.0033001881, LR: 0.0000000191\n",
      "Epoch: 592/600, Loss: 0.0032990373, LR: 0.0000000191\n",
      "Epoch: 593/600, Loss: 0.0032968958, LR: 0.0000000191\n",
      "Epoch: 594/600, Loss: 0.0032991099, LR: 0.0000000191\n",
      "Epoch: 595/600, Loss: 0.0032966112, LR: 0.0000000191\n",
      "Epoch: 596/600, Loss: 0.0032991994, LR: 0.0000000191\n",
      "Epoch: 597/600, Loss: 0.0032957737, LR: 0.0000000191\n",
      "Epoch: 598/600, Loss: 0.0032989022, LR: 0.0000000191\n",
      "Epoch: 599/600, Loss: 0.0033012035, LR: 0.0000000191\n",
      "Epoch: 600/600, Loss: 0.0032948778, LR: 0.0000000191\n"
     ]
    }
   ],
   "source": [
    "model = CursorRNN(input_size, hidden_size, output_size, num_layers=lstm_layers).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()  # loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # optimizer\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    ")  # learning rate scheduler for better convergence\n",
    "\n",
    "# for best model tracking\n",
    "best_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(\n",
    "            device\n",
    "        )  # moving batch to gpu or cpu based on availability\n",
    "        optimizer.zero_grad()  # zeroing the gradients\n",
    "\n",
    "        y_pred, _ = model(x_batch)  # forward pass\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)  # calculating loss\n",
    "        loss.backward()  # backpropagation\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), max_norm=1.0\n",
    "        )  # gradient clipping to prevent exploding gradients\n",
    "\n",
    "        optimizer.step()  # updating weights\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    scheduler.step(avg_loss)  # updating learning rate based on loss\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "\n",
    "    print(\n",
    "        f'Epoch: {epoch+1}/{epochs}, Loss: {avg_loss:.10f}, LR: {optimizer.param_groups[0][\"lr\"]:.10f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at /Users/arpanbhandari/Documents/coding/bumblebee/data/models/cursor-rnn-model-2025-03-19-16:53:33.pth\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"cursor-rnn-model-{time.strftime('%Y-%m-%d-%H:%M:%S')}.pth\"\n",
    "model_path = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"models\", model_name)\n",
    "torch.save(best_model_state, model_path)\n",
    "print(f\"Model saved at {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bumblebee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
